{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc1d Weights &amp; Biases Addons","text":"<p>Weights &amp; Biases Addons is a repository that provides of integrations and utilities that will supercharge your Weights &amp; Biases workflows. Its a repositpry built and maintained by <code>wandb</code> users for <code>wandb</code> users.</p>"},{"location":"#integrations","title":"Integrations","text":""},{"location":"#tensorflow-datasets","title":"TensorFlow Datasets","text":"<p>A set of utilities for easily accessing datasets for various machine learning tasks using Weights &amp; Biases artifacts built on top of TensorFlow Datasets.</p> <p>In order to install <code>wandb-addons</code> along with the dependencies for the dataset utilities, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[dataset]\n</code></pre> <ul> <li> <p><code>WandbDatasetBuilder</code>: An abstract class for Dataset builder that enables building a dataset and upload it as a Weights &amp; Biases Artifact.</p> </li> <li> <p><code>upload_dataset</code>: Upload and register a dataset with a TFDS module or a TFDS builder script as a Weights &amp; Biases artifact. This function would verify if a TFDS build/registration is possible with the current specified dataset path and upload it as a Weights &amp; Biases artifact.</p> </li> <li> <p><code>load_dataset</code>: Load a dataset from a wandb artifact. Using this function you can load a dataset hosted as a wandb artifact in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model.</p> </li> </ul>"},{"location":"#ciclo","title":"\ud83c\udf00 Ciclo","text":"<p>Functional callbacks for experiment tracking on Weights &amp; Biases with Ciclo.</p> <p>In order to install <code>wandb-addons</code> along with the dependencies for the ciclo callbacks, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[jax]\n</code></pre> <p>Once you've installed <code>wandb-addons</code>, you can import it using:</p> <pre><code>from wandb_addons.ciclo import WandbLogger\n</code></pre> <p>For more information, check out more at the docs.</p>"},{"location":"#monai","title":"MonAI","text":"<p>Event handlers for experiment tracking on Weights &amp; Biases with MonAI Engine for deep learning in healthcare imaging.</p> <p>In order to install <code>wandb-addons</code> along with the dependencies for the ciclo callbacks, you can run:</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons[monai]\n</code></pre> <p>Once you've installed <code>wandb-addons</code>, you can import it using:</p> <pre><code>from wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler\n</code></pre> <p>For more information, check out more at the docs.</p>"},{"location":"#converting-ipython-notebooks-to-reports","title":"Converting IPython Notebooks to Reports","text":"<p>A set of utilities to convert an IPython notebook to a Weights &amp; Biases report.</p> <p>Simply install <code>wandb-addons</code> using</p> <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install ./wandb-addons\n</code></pre> <p>You can convert your notebook to a report using either the Python function or the CLI:</p> CLIPython API <pre><code>nb2report \\\\\n--filepath Use_WandbMetricLogger_in_your_Keras_workflow.ipynb \\\\\n--wandb_project report-to-notebook \\\\\n--wandb_entity geekyrakshit \\\\\n--report_title \"Use WandbMetricLogger in your Keras Workflow\" \\\\\n--description \"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\" \\\\\n--width \"readable\"\n</code></pre> <pre><code>from wandb_addons.report import convert_to_wandb_report\nconvert_to_wandb_report(\nfilepath=\"Use_WandbMetricLogger_in_your_Keras_workflow.ipynb\",\nwandb_project=\"report-to-notebook\",\nwandb_entity=\"geekyrakshit\",\nreport_title=\"Use WandbMetricLogger in your Keras Workflow\",\ndescription=\"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\",\nwidth=\"readable\"\n)\n</code></pre> <p>For more information, check out more at the docs.</p>"},{"location":"#status","title":"Status","text":"<p><code>wandb-addons</code> is still in early development, the API for integrations and utilities is subject to change, expect things to break. If you are interested in contributing, please feel free to open an issue and/or raise a pull request.</p>"},{"location":"report/","title":"Notebook to Report Conversion","text":""},{"location":"report/#wandb_addons.report.notebook_convert.convert_to_wandb_report","title":"<code>convert_to_wandb_report(filepath, wandb_project, wandb_entity, report_title='Untitled Report', description='', width='readable')</code>","text":"<p>Convert an IPython Notebook to a Weights &amp; Biases Report.</p> <p>Usage:</p> CLIPython API <pre><code>nb2report \\\n--filepath Use_WandbMetricLogger_in_your_Keras_workflow.ipynb \\\n--wandb_project report-to-notebook \\\n--wandb_entity geekyrakshit \\\n--report_title \"Use WandbMetricLogger in your Keras Workflow\" \\\n--description \"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\" \\\n--width \"readable\"\n</code></pre> <pre><code>from wandb_addons.report import convert_to_wandb_report\nconvert_to_wandb_report(\nfilepath=\"Use_WandbMetricLogger_in_your_Keras_workflow.ipynb\",\nwandb_project=\"report-to-notebook\",\nwandb_entity=\"geekyrakshit\",\nreport_title=\"Use WandbMetricLogger in your Keras Workflow\",\ndescription=\"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\",\nwidth=\"readable\"\n)\n</code></pre> <p>Note</p> <p>In order to include panel grids with runsets and line plots in your report, you need to include YAML metadata regarding the runsets and line plots you want to include in a panel grid in a markdown cell of your notebook in the following format:</p> <pre><code>---\npanelgrid:\nrunsets:\n- project: report-to-notebook\nentity: geekyrakshit\nname: Training-Logs\nlineplots:\n- x: batch/batch_step\ny: batch/accuracy\n- x: batch/batch_step\ny: batch/learning_rate\n- x: batch/batch_step\ny: batch/loss\n- x: batch/batch_step\ny: batch/top@5_accuracy\n- x: epoch/epoch\ny: epoch/accuracy\n- x: epoch/epoch\ny: epoch/learning_rate\n- x: epoch/epoch\ny: epoch/loss\n- x: epoch/epoch\ny: epoch/top@5_accuracy\n- x: epoch/epoch\ny: epoch/val_accuracy\n- x: epoch/epoch\ny: epoch/val_loss\n- x: epoch/epoch\ny: epoch/val_top@5_accuracy\n---\n</code></pre> <p>Currently only line plots are supported inside panel grids.</p> <p>Converting using CLI</p> <p>Convert an IPython notebook to a Weights &amp; Biases report using the <code>nb2report</code> CLI:</p> <pre><code>Usage: nb2report [OPTIONS]\nOptions:\n--filepath TEXT       Path to an IPython notebook\n--wandb_project TEXT  The name of the Weights &amp; Biases project where you're\n                        creating the project\n--wandb_entity TEXT   The name of the Weights &amp; Biases entity (username or\n                        team name)\n--report_title TEXT   The title of the report\n--description TEXT    The description of the report\n--width TEXT          Width of the report, one of `'readable'`, `'fixed'`,\n                        or `'fluid'`\n--help                Show this message and exit.\n</code></pre> <p>Example</p> <p>Here is a report was generated for this notebook.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to an IPython notebook.</p> required <code>wandb_project</code> <code>str</code> <p>The name of the Weights &amp; Biases project where you're creating the project.</p> required <code>wandb_entity</code> <code>str</code> <p>The name of the Weights &amp; Biases entity (username or team name).</p> required <code>report_title</code> <code>Optional[str]</code> <p>The title of the report.</p> <code>'Untitled Report'</code> <code>description</code> <code>Optional[str]</code> <p>The description of the report.</p> <code>''</code> <code>width</code> <code>Optional[str]</code> <p>Width of the report, one of <code>\"readable\"</code>, <code>\"fixed\"</code>, or <code>\"fluid\"</code>.</p> <code>'readable'</code> Source code in <code>wandb_addons/report/notebook_convert.py</code> <pre><code>def convert_to_wandb_report(\nfilepath: str,\nwandb_project: str,\nwandb_entity: str,\nreport_title: Optional[str] = \"Untitled Report\",\ndescription: Optional[str] = \"\",\nwidth: Optional[str] = \"readable\",\n):\n\"\"\"Convert an IPython Notebook to a [Weights &amp; Biases Report](https://docs.wandb.ai/guides/reports).\n    **Usage:**\n    === \"CLI\"\n        ```shell\n        nb2report \\\\\n            --filepath Use_WandbMetricLogger_in_your_Keras_workflow.ipynb \\\\\n            --wandb_project report-to-notebook \\\\\n            --wandb_entity geekyrakshit \\\\\n            --report_title \"Use WandbMetricLogger in your Keras Workflow\" \\\\\n            --description \"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\" \\\\\n            --width \"readable\"\n        ```\n    === \"Python API\"\n        ```python\n        from wandb_addons.report import convert_to_wandb_report\n        convert_to_wandb_report(\n            filepath=\"Use_WandbMetricLogger_in_your_Keras_workflow.ipynb\",\n            wandb_project=\"report-to-notebook\",\n            wandb_entity=\"geekyrakshit\",\n            report_title=\"Use WandbMetricLogger in your Keras Workflow\",\n            description=\"A guide to using the WandbMetricLogger callback in your Keras and TensorFlow training worflow\",\n            width=\"readable\"\n        )\n        ```\n    !!! note\n        In order to include panel grids with runsets and line plots in your report, you need to include\n        YAML metadata regarding the runsets and line plots you want to include in a panel grid in a\n        markdown cell of your notebook in the following format:\n        ```yaml\n        ---\n        panelgrid:\n        runsets:\n        - project: report-to-notebook\n            entity: geekyrakshit\n            name: Training-Logs\n        lineplots:\n        - x: batch/batch_step\n            y: batch/accuracy\n        - x: batch/batch_step\n            y: batch/learning_rate\n        - x: batch/batch_step\n            y: batch/loss\n        - x: batch/batch_step\n            y: batch/top@5_accuracy\n        - x: epoch/epoch\n            y: epoch/accuracy\n        - x: epoch/epoch\n            y: epoch/learning_rate\n        - x: epoch/epoch\n            y: epoch/loss\n        - x: epoch/epoch\n            y: epoch/top@5_accuracy\n        - x: epoch/epoch\n            y: epoch/val_accuracy\n        - x: epoch/epoch\n            y: epoch/val_loss\n        - x: epoch/epoch\n            y: epoch/val_top@5_accuracy\n        ---\n        ```\n        Currently only line plots are supported inside panel grids.\n    !!! note \"Converting using CLI\"\n        Convert an IPython notebook to a Weights &amp; Biases report using the `nb2report` CLI:\n        ```shell\n        Usage: nb2report [OPTIONS]\n        Options:\n        --filepath TEXT       Path to an IPython notebook\n        --wandb_project TEXT  The name of the Weights &amp; Biases project where you're\n                                creating the project\n        --wandb_entity TEXT   The name of the Weights &amp; Biases entity (username or\n                                team name)\n        --report_title TEXT   The title of the report\n        --description TEXT    The description of the report\n        --width TEXT          Width of the report, one of `'readable'`, `'fixed'`,\n                                or `'fluid'`\n        --help                Show this message and exit.\n        ```\n    !!! example \"Example\"\n        [Here](https://wandb.ai/geekyrakshit/report-to-notebook/reports/Use-WandbMetricLogger-in-your-Keras-Workflow--Vmlldzo0Mjg4NTM2) is a report was generated for [this](https://github.com/wandb/examples/blob/master/colabs/keras/Use_WandbMetricLogger_in_your_Keras_workflow.ipynb) notebook.\n    Args:\n        filepath (str): Path to an IPython notebook.\n        wandb_project (str): The name of the Weights &amp; Biases project where you're creating the project.\n        wandb_entity (str): The name of the Weights &amp; Biases entity (username or team name).\n        report_title (Optional[str]): The title of the report.\n        description (Optional[str]): The description of the report.\n        width (Optional[str]): Width of the report, one of `\"readable\"`, `\"fixed\"`, or `\"fluid\"`.\n    \"\"\"\nnotebook_cells = _parse_notebook_cells(filepath)\nreport = wr.Report(\nproject=wandb_project,\ntitle=report_title,\nentity=wandb_entity,\ndescription=description,\nwidth=width,\n)\nblocks = []\nfor cell in tqdm(notebook_cells, desc=\"Converting notebook cells to report cells\"):\nif cell[\"type\"] == \"markdown\":\nblocks.append(wr.MarkdownBlock(text=cell[\"source\"]))\nelif cell[\"type\"] == \"code\":\nblocks.append(wr.MarkdownBlock(text=f\"```python\\n{cell['source']}\\n```\"))\nelif cell[\"type\"] == \"panel_metadata\":\nblocks.append(_convert_metadata_to_panelgrid(metadata=cell[\"source\"]))\nreport.blocks = blocks\nreport.save()\nwandb.termlog(\n\"Report {report_title} created successfully. \"\n+ f\"Check list of reports at {report.url}.\"\n)\n</code></pre>"},{"location":"utils/","title":"Utilities","text":""},{"location":"utils/#wandb_addons.utils.fetch_wandb_artifact","title":"<code>fetch_wandb_artifact(artifact_address, artifact_type)</code>","text":"<p>Utility function for fetching a Weights &amp; Biases artifact irrespective of whether a run has been initialized or not.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_address</code> <code>str</code> <p>A human-readable name for the artifact, which is how you can identify the artifact in the UI or reference it in <code>use_artifact</code> calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project.</p> required <code>artifact_type</code> <code>str</code> <p>The type of the artifact, which is used to organize and differentiate artifacts. Common typesCinclude dataset or model, but you can use any string containing letters, numbers, underscores, hyphens, and dots.</p> required <p>Returns:</p> Type Description <code>wandb.util.FilePathStr</code> <p>The path to the downloaded contents.</p> Source code in <code>wandb_addons/utils.py</code> <pre><code>def fetch_wandb_artifact(artifact_address: str, artifact_type: str) -&gt; FilePathStr:\n\"\"\"Utility function for fetching a\n    [Weights &amp; Biases artifact](https://docs.wandb.ai/guides/artifacts)\n    irrespective of whether a [run](https://docs.wandb.ai/guides/runs) has been initialized or not.\n    Args:\n        artifact_address (str): A human-readable name for the artifact, which is how you can\n            identify the artifact in the UI or reference it in\n            [`use_artifact`](https://docs.wandb.ai/ref/python/run#use_artifact) calls. Names can\n            contain letters, numbers, underscores, hyphens, and dots. The name must be unique across\n            a project.\n        artifact_type (str): The type of the artifact, which is used to organize and differentiate\n            artifacts. Common typesCinclude dataset or model, but you can use any string containing\n            letters, numbers, underscores, hyphens, and dots.\n    Returns:\n        (wandb.util.FilePathStr): The path to the downloaded contents.\n    \"\"\"\nreturn (\nwandb.Api().artifact(artifact_address, type=artifact_type).download()\nif wandb.run is None\nelse wandb.use_artifact(artifact_address, type=artifact_type).download()\n)\n</code></pre>"},{"location":"utils/#wandb_addons.utils.flatten_nested_dictionaries","title":"<code>flatten_nested_dictionaries(d, parent_key='', sep='/')</code>","text":"<p>A recursive function for flattening nested dictionaries.</p>"},{"location":"utils/#wandb_addons.utils.flatten_nested_dictionaries--reference","title":"Reference:","text":"<pre><code>Answer to\n[**Flatten nested dictionaries, compressing keys**](https://stackoverflow.com/q/6027558)\non StackOverflow: [stackoverflow.com/a/6027615](https://stackoverflow.com/a/6027615)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Dict</code> <p>The input nested dictionary.</p> required <code>parent_key</code> <code>str</code> <p>The parent key.</p> <code>''</code> <code>sep</code> <code>str</code> <p>The separator to use for the flattened keys.</p> <code>'/'</code> <p>Returns:</p> Type Description <code>Dict</code> <p>The flattened dictionary.</p> Source code in <code>wandb_addons/utils.py</code> <pre><code>def flatten_nested_dictionaries(d: Dict, parent_key: str = \"\", sep: str = \"/\") -&gt; Dict:\n\"\"\"A recursive function for flattening nested dictionaries.\n    # Reference:\n        Answer to\n        [**Flatten nested dictionaries, compressing keys**](https://stackoverflow.com/q/6027558)\n        on StackOverflow: [stackoverflow.com/a/6027615](https://stackoverflow.com/a/6027615)\n    Args:\n        d (Dict): The input nested dictionary.\n        parent_key (str): The parent key.\n        sep (str): The separator to use for the flattened keys.\n    Returns:\n        (Dict): The flattened dictionary.\n    \"\"\"\nitems = []\nfor k, v in d.items():\nnew_key = parent_key + sep + k if parent_key else k\nif isinstance(v, MutableMapping):\nitems.extend(flatten_nested_dictionaries(v, new_key, sep=sep).items())\nelse:\nitems.append((new_key, v))\nreturn dict(items)\n</code></pre>"},{"location":"ciclo/ciclo/","title":"Ciclo Callbacks","text":"<p>Functional callbacks for experiment tracking on Weights &amp; Biases with Ciclo.</p>"},{"location":"ciclo/ciclo/#wandb_addons.ciclo.wandb_log.WandbLogger","title":"<code>WandbLogger</code>","text":"<p>         Bases: <code>LoopCallbackBase[S]</code></p> <p>A ciclo callback for logging to Weights &amp; Biases.</p> Example notebooks: <ul> <li>MNIST classification using Ciclo.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>additional_logging</code> <code>Optional[Callable]</code> <p>A function to be called after each logging step and can be used to log additional values or media to Weights &amp; Biases.</p> <code>None</code> Source code in <code>wandb_addons/ciclo/wandb_log.py</code> <pre><code>class WandbLogger(LoopCallbackBase[S]):\n\"\"\"A [ciclo](https://github.com/cgarciae/ciclo) callback for logging to Weights &amp; Biases.\n    ??? example \"Example notebooks:\"\n        - [MNIST classification using Ciclo](../examples/ciclo_MNIST).\n    Args:\n        additional_logging (Optional[Callable]): A function to be called after each logging step\n            and can be used to log additional values or media to Weights &amp; Biases.\n    \"\"\"\ndef __init__(self, additional_logging: Optional[Callable] = None):\nself.additional_logging = additional_logging\ndef __call__(self, elapsed: Elapsed, state: S, logs: Optional[Logs] = None):\nwandb.log(flatten_nested_dictionaries(logs, sep=\"/\"))\nif self.additional_logging is not None:\nself.additional_logging()\ndef __loop_callback__(self, loop_state: LoopState[S]) -&gt; CallbackOutput[S]:\nself(loop_state.elapsed, loop_state.state, loop_state.accumulated_logs)\nreturn Logs(), loop_state.state\ndef on_epoch_end(\nself, state, batch, elapsed, loop_state: LoopState[S]\n) -&gt; CallbackOutput[S]:\nreturn self.__loop_callback__(loop_state)\n</code></pre>"},{"location":"ciclo/examples/ciclo_MNIST/","title":"ciclo MNIST","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/soumik12345/wandb-addons\n!pip install .[jax]\n</pre> !git clone https://github.com/soumik12345/wandb-addons !pip install .[jax] In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\nfrom time import time\nfrom typing import Optional, Callable\nfrom collections.abc import MutableMapping\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nimport jax_metrics as jm\nimport matplotlib.pyplot as plt\nimport optax\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport ciclo\nfrom ciclo.logging import Logs\nfrom ciclo.types import Batch, S\nfrom ciclo.timetracking import Elapsed\nfrom ciclo.loops.loop import LoopCallbackBase\nfrom ciclo.callbacks import LoopState, CallbackOutput\n\nimport wandb\nfrom wandb_addons.ciclo import WandbLogger\n</pre> from pathlib import Path from time import time from typing import Optional, Callable from collections.abc import MutableMapping  import flax.linen as nn import jax.numpy as jnp import jax_metrics as jm import matplotlib.pyplot as plt import optax import tensorflow as tf import tensorflow_datasets as tfds  import ciclo from ciclo.logging import Logs from ciclo.types import Batch, S from ciclo.timetracking import Elapsed from ciclo.loops.loop import LoopCallbackBase from ciclo.callbacks import LoopState, CallbackOutput  import wandb from wandb_addons.ciclo import WandbLogger In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"ciclo-integration\", entity=\"geekyrakshit\", job_type=\"test\")\n</pre> wandb.init(project=\"ciclo-integration\", entity=\"geekyrakshit\", job_type=\"test\") In\u00a0[\u00a0]: Copied! <pre>batch_size = 32\ntotal_samples = 32 * 100\ntotal_steps = total_samples // batch_size\nsteps_per_epoch = total_steps // 10\ntest_steps = 10\n</pre> batch_size = 32 total_samples = 32 * 100 total_steps = total_samples // batch_size steps_per_epoch = total_steps // 10 test_steps = 10 In\u00a0[\u00a0]: Copied! <pre># load the MNIST dataset\nds_train: tf.data.Dataset = tfds.load(\"mnist\", split=\"train\", shuffle_files=True)\nds_train = ds_train.map(lambda x: (x[\"image\"], x[\"label\"]))\nds_train = ds_train.repeat().shuffle(1024).batch(batch_size).prefetch(1)\nds_test: tf.data.Dataset = tfds.load(\"mnist\", split=\"test\")\nds_test = ds_test.map(lambda x: (x[\"image\"], x[\"label\"]))  # .take(10)\nds_test = ds_test.batch(32, drop_remainder=True).prefetch(1)\n</pre> # load the MNIST dataset ds_train: tf.data.Dataset = tfds.load(\"mnist\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(lambda x: (x[\"image\"], x[\"label\"])) ds_train = ds_train.repeat().shuffle(1024).batch(batch_size).prefetch(1) ds_test: tf.data.Dataset = tfds.load(\"mnist\", split=\"test\") ds_test = ds_test.map(lambda x: (x[\"image\"], x[\"label\"]))  # .take(10) ds_test = ds_test.batch(32, drop_remainder=True).prefetch(1) In\u00a0[\u00a0]: Copied! <pre># Define model\nclass Linear(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = x / 255.0\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = nn.Dense(features=10)(x)\n        return x\n</pre> # Define model class Linear(nn.Module):     @nn.compact     def __call__(self, x):         x = x / 255.0         x = x.reshape((x.shape[0], -1))  # flatten         x = nn.Dense(features=10)(x)         return x In\u00a0[\u00a0]: Copied! <pre># Initialize state\nmodel = Linear()\nstate = ciclo.create_flax_state(\n    model,\n    inputs=jnp.empty((1, 28, 28, 1)),\n    tx=optax.adamw(1e-3),\n    losses={\"loss\": jm.losses.Crossentropy()},\n    metrics={\"accuracy\": jm.metrics.Accuracy()},\n    strategy=\"jit\",\n)\n</pre> # Initialize state model = Linear() state = ciclo.create_flax_state(     model,     inputs=jnp.empty((1, 28, 28, 1)),     tx=optax.adamw(1e-3),     losses={\"loss\": jm.losses.Crossentropy()},     metrics={\"accuracy\": jm.metrics.Accuracy()},     strategy=\"jit\", ) In\u00a0[\u00a0]: Copied! <pre>state, history, _ = ciclo.train_loop(\n    state,\n    ds_train.as_numpy_iterator(),\n    callbacks=[\n        ciclo.keras_bar(total=total_steps),\n        ciclo.checkpoint(\n            f\"logdir/checkpoint/{int(time())}\",\n            monitor=\"accuracy_test\",\n            mode=\"max\",\n        ),\n        WandbLogger(),\n    ],\n    test_dataset=lambda: ds_test.as_numpy_iterator(),\n    epoch_duration=steps_per_epoch,\n    test_duration=test_steps,\n    stop=total_steps,\n)\n</pre> state, history, _ = ciclo.train_loop(     state,     ds_train.as_numpy_iterator(),     callbacks=[         ciclo.keras_bar(total=total_steps),         ciclo.checkpoint(             f\"logdir/checkpoint/{int(time())}\",             monitor=\"accuracy_test\",             mode=\"max\",         ),         WandbLogger(),     ],     test_dataset=lambda: ds_test.as_numpy_iterator(),     epoch_duration=steps_per_epoch,     test_duration=test_steps,     stop=total_steps, ) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"dataset/dataset_loading/","title":"Weights &amp; Biases Dataloader","text":"<p>A set of utilities for easily accessing datasets for various machine learning tasks using Weights &amp; Biases artifacts.</p>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_builder.WandbDatasetBuilder","title":"<code>WandbDatasetBuilder</code>","text":"<p>         Bases: <code>tfds.core.GeneratorBasedBuilder</code></p> <p>An abstract class for Dataset builder that enables building a dataset and upload it as a Weights &amp; Biases Artifact. It expects subclasses to override the following functions:</p> <ul> <li> <p><code>_split_generators</code> to return a dict of splits, generators.</p> </li> <li> <p><code>_generate_examples</code> to return a generator or an iterator corresponding to the split.</p> </li> </ul> <p>Note</p> <p>Note that this process is alternative to the dataset preparation process using tfds module described here. The dataset registered and uploaded using both approaches is easily consumable using the fuction <code>load_dataset</code>.</p> <p>Example Artifacts</p> <ul> <li>\ud83d\udc12 Monkey Dataset.</li> </ul> <p>Usage:</p> <pre><code>import os\nfrom glob import glob\nfrom typing import Any, Mapping, Optional, Union\nfrom etils import epath\nimport tensorflow_datasets as tfds\nimport wandb\nfrom wandb_addons.dataset import WandbDatasetBuilder\nclass MonkeyDatasetBuilder(WandbDatasetBuilder):\ndef __init__(\nself,\n*,\nname: str,\ndataset_path: str,\nfeatures: tfds.features.FeatureConnector,\nupload_raw_dataset: bool = True,\nconfig: Union[None, str, tfds.core.BuilderConfig] = None,\ndata_dir: Optional[epath.PathLike] = None,\ndescription: Optional[str] = None,\nrelease_notes: Optional[Mapping[str, str]] = None,\nhomepage: Optional[str] = None,\nfile_format: Optional[Union[str, tfds.core.FileFormat]] = None,\ndisable_shuffling: Optional[bool] = False,\n**kwargs: Any,\n):\nsuper().__init__(\nname=name,\ndataset_path=dataset_path,\nfeatures=features,\nupload_raw_dataset=upload_raw_dataset,\nconfig=config,\ndescription=description,\ndata_dir=data_dir,\nrelease_notes=release_notes,\nhomepage=homepage,\nfile_format=file_format,\ndisable_shuffling=disable_shuffling,\n)\ndef _split_generators(self, dl_manager: tfds.download.DownloadManager):\nreturn {\n\"train\": self._generate_examples(\nos.path.join(self.dataset_path, \"training\", \"training\")\n),\n\"val\": self._generate_examples(\nos.path.join(self.dataset_path, \"validation\", \"validation\")\n),\n}\ndef _generate_examples(self, path):\nimage_paths = glob(os.path.join(path, \"*\", \"*.jpg\"))\nfor image_path in image_paths:\nlabel = _CLASS_LABELS[int(image_path.split(\"/\")[-2][-1])]\nyield image_path, {\n\"image\": image_path,\n\"label\": label,\n}\nif __name__ == \"__main__\":\nwandb.init(project=\"artifact-accessor\", entity=\"geekyrakshit\")\nbuilder = MonkeyDatasetBuilder(\nname=\"monkey_dataset\",\ndataset_path=\"path/to/my/datase\",\nfeatures=tfds.features.FeaturesDict(\n{\n\"image\": tfds.features.Image(shape=(None, None, 3)),\n\"label\": tfds.features.ClassLabel(names=_CLASS_LABELS),\n}\n),\ndata_dir=\"build_dir/\",\ndescription=_DESCRIPTION,\n)\nbuilder.build_and_upload(create_visualizations=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A human-readable name for this artifact, which is how you can identify this artifact in the UI or reference it in <code>use_artifact</code> calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset.</p> required <code>features</code> <code>tfds.features.FeatureConnector</code> <p>The dataset feature types. Refer to the <code>tfds.features</code> module for more information.</p> required <code>upload_raw_dataset</code> <code>Optional[bool]</code> <p>Whether to upload the raw dataset to Weights &amp; Biases artifacts as well or not. If set to <code>True</code>, the dataset builder would upload the raw dataset besides the built dataset, as different versions of the same artifact; with the raw dataset being the lower version.</p> <code>False</code> <code>config</code> <code>Union[None, str, tfds.core.BuilderConfig]</code> <p>Dataset configuration.</p> <code>None</code> <code>data_dir</code> <code>Optional[epath.PathLike]</code> <p>The directory where the dataset will be built.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Description of the dataset as a valid markdown string.</p> <code>None</code> <code>release_notes</code> <code>Optional[Mapping[str, str]]</code> <p>Release notes.</p> <code>None</code> <code>homepage</code> <code>Optional[str]</code> <p>Homepage of the dataset.</p> <code>None</code> <code>file_format</code> <code>Optional[Union[str, tfds.core.FileFormat]]</code> <p>EXPERIMENTAL, may change at any time; Format of the record files in which dataset will be read/written to. If <code>None</code>, defaults to <code>tfrecord</code>.</p> <code>None</code> <code>disable_shuffling</code> <code>Optional[bool]</code> <p>Disable shuffling of the dataset order.</p> <code>True</code> Source code in <code>wandb_addons/dataset/dataset_builder.py</code> <pre><code>class WandbDatasetBuilder(tfds.core.GeneratorBasedBuilder):\n\"\"\"An abstract class for Dataset builder that enables building a dataset and upload it as a\n    [Weights &amp; Biases Artifact](https://docs.wandb.ai/guides/artifacts). It expects subclasses\n    to override the following functions:\n    - **`_split_generators`** to return a dict of splits, generators.\n    - **`_generate_examples`** to return a generator or an iterator corresponding to the split.\n    !!! note \"Note\"\n        Note that this process is alternative to the dataset preparation process using tfds module\n        described [here](../dataset_preparation). The dataset registered and uploaded using both\n        approaches is easily consumable using the fuction\n        [`load_dataset`](./#wandb_addons.dataset.dataset_loading.load_dataset).\n    !!! example \"Example Artifacts\"\n        - [\ud83d\udc12 Monkey Dataset](https://wandb.ai/geekyrakshit/artifact-accessor/artifacts/dataset/monkey_dataset).\n    **Usage:**\n    ```python\n    import os\n    from glob import glob\n    from typing import Any, Mapping, Optional, Union\n    from etils import epath\n    import tensorflow_datasets as tfds\n    import wandb\n    from wandb_addons.dataset import WandbDatasetBuilder\n    class MonkeyDatasetBuilder(WandbDatasetBuilder):\n        def __init__(\n            self,\n            *,\n            name: str,\n            dataset_path: str,\n            features: tfds.features.FeatureConnector,\n            upload_raw_dataset: bool = True,\n            config: Union[None, str, tfds.core.BuilderConfig] = None,\n            data_dir: Optional[epath.PathLike] = None,\n            description: Optional[str] = None,\n            release_notes: Optional[Mapping[str, str]] = None,\n            homepage: Optional[str] = None,\n            file_format: Optional[Union[str, tfds.core.FileFormat]] = None,\n            disable_shuffling: Optional[bool] = False,\n            **kwargs: Any,\n        ):\n            super().__init__(\n                name=name,\n                dataset_path=dataset_path,\n                features=features,\n                upload_raw_dataset=upload_raw_dataset,\n                config=config,\n                description=description,\n                data_dir=data_dir,\n                release_notes=release_notes,\n                homepage=homepage,\n                file_format=file_format,\n                disable_shuffling=disable_shuffling,\n            )\n        def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n            return {\n                \"train\": self._generate_examples(\n                    os.path.join(self.dataset_path, \"training\", \"training\")\n                ),\n                \"val\": self._generate_examples(\n                    os.path.join(self.dataset_path, \"validation\", \"validation\")\n                ),\n            }\n        def _generate_examples(self, path):\n            image_paths = glob(os.path.join(path, \"*\", \"*.jpg\"))\n            for image_path in image_paths:\n                label = _CLASS_LABELS[int(image_path.split(\"/\")[-2][-1])]\n                yield image_path, {\n                    \"image\": image_path,\n                    \"label\": label,\n                }\n    if __name__ == \"__main__\":\n        wandb.init(project=\"artifact-accessor\", entity=\"geekyrakshit\")\n        builder = MonkeyDatasetBuilder(\n            name=\"monkey_dataset\",\n            dataset_path=\"path/to/my/datase\",\n            features=tfds.features.FeaturesDict(\n                {\n                    \"image\": tfds.features.Image(shape=(None, None, 3)),\n                    \"label\": tfds.features.ClassLabel(names=_CLASS_LABELS),\n                }\n            ),\n            data_dir=\"build_dir/\",\n            description=_DESCRIPTION,\n        )\n        builder.build_and_upload(create_visualizations=True)\n    ```\n    Args:\n        name (str): A human-readable name for this artifact, which is how you can identify this\n            artifact in the UI or reference it in\n            [`use_artifact`](https://docs.wandb.ai/ref/python/run#use_artifact) calls. Names can\n            contain letters, numbers, underscores, hyphens, and dots. The name must be unique\n            across a project.\n        dataset_path (str): Path to the dataset.\n        features (tfds.features.FeatureConnector): The dataset feature types. Refer to the\n            [`tfds.features`](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/)\n            module for more information.\n        upload_raw_dataset (Optional[bool]): Whether to upload the raw dataset to Weights &amp; Biases\n            artifacts as well or not. If set to `True`, the dataset builder would upload the raw\n            dataset besides the built dataset, as different versions of the same artifact; with the\n            raw dataset being the lower version.\n        config (Union[None, str, tfds.core.BuilderConfig]): Dataset configuration.\n        data_dir (Optional[epath.PathLike]): The directory where the dataset will be built.\n        description (Optional[str]): Description of the dataset as a valid markdown string.\n        release_notes (Optional[Mapping[str, str]]): Release notes.\n        homepage (Optional[str]): Homepage of the dataset.\n        file_format (Optional[Union[str, tfds.core.FileFormat]]): **EXPERIMENTAL**, may change at any\n            time; Format of the record files in which dataset will be read/written to. If `None`,\n            defaults to `tfrecord`.\n        disable_shuffling (Optional[bool]): Disable shuffling of the dataset order.\n    \"\"\"\ndef __init__(\nself,\n*,\nname: str,\ndataset_path: str,\nfeatures: tfds.features.FeatureConnector,\nupload_raw_dataset: Optional[bool] = False,\nconfig: Union[None, str, tfds.core.BuilderConfig] = None,\ndata_dir: Optional[epath.PathLike] = None,\ndescription: Optional[str] = None,\nrelease_notes: Optional[Mapping[str, str]] = None,\nhomepage: Optional[str] = None,\nfile_format: Optional[Union[str, tfds.core.FileFormat]] = None,\ndisable_shuffling: Optional[bool] = True,\n**kwargs: Any,\n):\nif wandb.run is None:\nraise wandb.Error(\n\"You must call `wandb.init()` before instantiating a subclass of `WandbDatasetBuilder`\"\n)\nself.name = name\nself.dataset_path = dataset_path\nself.upload_raw_dataset = upload_raw_dataset\nself.VERSION = self._get_version()\nself.RELEASE_NOTES = release_notes\nif config:\nif isinstance(config, str):\nconfig = tfds.core.BuilderConfig(\nname=config, version=self.VERSION, release_notes=release_notes\n)\nself.BUILDER_CONFIGS = [config]\nself._feature_spec = features\nself._description = (\ndescription or \"Dataset built without a DatasetBuilder class.\"\n)\nself._homepage = homepage\nself._disable_shuffling = disable_shuffling\nself._initialize_wandb_artifact()\nsuper().__init__(\ndata_dir=data_dir,\nconfig=config,\nversion=self.VERSION,\nfile_format=file_format,\n**kwargs,\n)\ndef _initialize_wandb_artifact(self):\nmetadata = {\n\"description\": self._description,\n\"release-notes\": self.RELEASE_NOTES,\n\"homepage\": self._homepage,\n}\nif self.upload_raw_dataset:\nself._wandb_raw_artifact = wandb.Artifact(\nname=self.name,\ntype=\"dataset\",\ndescription=self._description,\nmetadata=metadata,\n)\nself._wandb_build_artifact = wandb.Artifact(\nname=self.name,\ntype=\"dataset\",\ndescription=self._description,\nmetadata=metadata,\n)\ndef _get_version(self) -&gt; tfds.core.utils.Version:\ntry:\napi = wandb.Api()\nversions = api.artifact_versions(\ntype_name=\"dataset\",\nname=f\"{wandb.run.entity}/{wandb.run.project}/{self.name}\",\n)\nversion = int(next(versions).source_version[1:])\nversion = version + 1 if self.upload_raw_dataset else version\nreturn str(version) + \".0.0\"\nexcept wandb.errors.CommError:\nversion = 1 if self.upload_raw_dataset else 0\nreturn tfds.core.utils.Version(str(version) + \".0.0\")\ndef _info(self) -&gt; tfds.core.DatasetInfo:\nreturn tfds.core.DatasetInfo(\nbuilder=self,\ndescription=self._description,\nfeatures=self._feature_spec,\nhomepage=self._homepage,\ndisable_shuffling=self._disable_shuffling,\n)\ndef _create_report(self):\nreport = wr.Report(project=wandb.run.project)\ndataset_splits = flatten_nested_dictionaries(self.info.splits).keys()\nscalar_panels = [\nwr.ScalarChart(\ntitle=f\"{split}/num_examples\", metric=f\"{split}/num_examples\"\n)\nfor split in dataset_splits\n]\nscalar_panels += [\nwr.ScalarChart(title=f\"{split}/num_shards\", metric=f\"{split}/num_shards\")\nfor split in dataset_splits\n]\nreport.title = f\"Dataset: {self.name}\"\nreport.blocks = [\nwr.MarkdownBlock(\ntext=f\"**Disclaimer:** This report was generated automatically.\"\n),\nwr.H1(\"Description\"),\nwr.MarkdownBlock(text=self._description),\nwr.MarkdownBlock(\"\\n\"),\n]\nif self._homepage is not None:\nreport.blocks += [\nwr.MarkdownBlock(text=f\"**Homepage:** {self._homepage}\"),\nwr.MarkdownBlock(\"\\n\"),\n]\nreport.blocks += [\nwr.MarkdownBlock(\nf\"\"\"\n            ```python\n            import from wandb_addons.dataset import load_dataset\n            datasets, dataset_builder_info = load_dataset(\"{wandb.run.entity}/{wandb.run.project}/{self.name}:tfrecord\")\n            ```\n            \"\"\"\n),\nwr.MarkdownBlock(\"\\n\"),\n]\nreport.blocks += [\nwr.PanelGrid(\nrunsets=[wr.Runset(project=wandb.run.project, entity=wandb.run.entity)],\npanels=scalar_panels\n+ [\nwr.WeavePanelSummaryTable(table_name=f\"{self.name}-Table\"),\nwr.WeavePanelArtifact(self.name),\n],\n)\n]\nreport.save()\ndef build_and_upload(\nself,\ncreate_visualizations: bool = False,\nmax_visualizations_per_split: Optional[int] = None,\n):\n\"\"\"Build and prepare the dataset for loading and uploads as a\n        [Weights &amp; Biases Artifact](https://docs.wandb.ai/guides/artifacts). This function also\n        creates a Weights &amp; Biases reports that contains the dataset description, visualizations\n        and all additional metadata logged to Weights &amp; Biases.\n        !!! example \"Sample Auto-generated Report\"\n            [\ud83d\udc12 Dataset: monkey-dataset](https://wandb.ai/geekyrakshit/artifact-accessor/reports/Dataset-monkey-dataset--Vmlldzo0MjgxNTAz)\n        Args:\n            create_visualizations (bool): Automatically parse the dataset and visualize using a\n                [Weights &amp; Biase Table](https://docs.wandb.ai/guides/data-vis).\n            max_visualizations_per_split (Optional[int]): Maximum number of visualizations per\n                split to be visualized in WandB Table. By default, the whole dataset is visualized.\n        \"\"\"\nsuper().download_and_prepare()\nif create_visualizations:\ntable_creator = TableCreator(\ndataset_builder=self,\ndataset_info=self.info,\nmax_visualizations_per_split=max_visualizations_per_split,\n)\ntable_creator.populate_table()\ntable_creator.log(dataset_name=self.name)\nif self.upload_raw_dataset:\nself._wandb_raw_artifact.add_dir(self.dataset_path)\nwandb.log_artifact(self._wandb_raw_artifact, aliases=[\"raw\"])\nself._wandb_build_artifact.add_dir(self.data_dir)\nwandb.log_artifact(self._wandb_build_artifact, aliases=[\"tfrecord\"])\nself._create_report()\n</code></pre>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_builder.WandbDatasetBuilder.build_and_upload","title":"<code>build_and_upload(create_visualizations=False, max_visualizations_per_split=None)</code>","text":"<p>Build and prepare the dataset for loading and uploads as a Weights &amp; Biases Artifact. This function also creates a Weights &amp; Biases reports that contains the dataset description, visualizations and all additional metadata logged to Weights &amp; Biases.</p> <p>Sample Auto-generated Report</p> <p>\ud83d\udc12 Dataset: monkey-dataset</p> <p>Parameters:</p> Name Type Description Default <code>create_visualizations</code> <code>bool</code> <p>Automatically parse the dataset and visualize using a Weights &amp; Biase Table.</p> <code>False</code> <code>max_visualizations_per_split</code> <code>Optional[int]</code> <p>Maximum number of visualizations per split to be visualized in WandB Table. By default, the whole dataset is visualized.</p> <code>None</code> Source code in <code>wandb_addons/dataset/dataset_builder.py</code> <pre><code>def build_and_upload(\nself,\ncreate_visualizations: bool = False,\nmax_visualizations_per_split: Optional[int] = None,\n):\n\"\"\"Build and prepare the dataset for loading and uploads as a\n    [Weights &amp; Biases Artifact](https://docs.wandb.ai/guides/artifacts). This function also\n    creates a Weights &amp; Biases reports that contains the dataset description, visualizations\n    and all additional metadata logged to Weights &amp; Biases.\n    !!! example \"Sample Auto-generated Report\"\n        [\ud83d\udc12 Dataset: monkey-dataset](https://wandb.ai/geekyrakshit/artifact-accessor/reports/Dataset-monkey-dataset--Vmlldzo0MjgxNTAz)\n    Args:\n        create_visualizations (bool): Automatically parse the dataset and visualize using a\n            [Weights &amp; Biase Table](https://docs.wandb.ai/guides/data-vis).\n        max_visualizations_per_split (Optional[int]): Maximum number of visualizations per\n            split to be visualized in WandB Table. By default, the whole dataset is visualized.\n    \"\"\"\nsuper().download_and_prepare()\nif create_visualizations:\ntable_creator = TableCreator(\ndataset_builder=self,\ndataset_info=self.info,\nmax_visualizations_per_split=max_visualizations_per_split,\n)\ntable_creator.populate_table()\ntable_creator.log(dataset_name=self.name)\nif self.upload_raw_dataset:\nself._wandb_raw_artifact.add_dir(self.dataset_path)\nwandb.log_artifact(self._wandb_raw_artifact, aliases=[\"raw\"])\nself._wandb_build_artifact.add_dir(self.data_dir)\nwandb.log_artifact(self._wandb_build_artifact, aliases=[\"tfrecord\"])\nself._create_report()\n</code></pre>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_upload.upload_dataset","title":"<code>upload_dataset(dataset_name, dataset_path, aliases=None, upload_tfrecords=True, quiet=False)</code>","text":"<p>Upload and register a dataset with a TFDS module or a TFDS builder script as a Weights &amp; Biases artifact. This function would verify if a TFDS build/registration is possible with the current specified dataset path and upload it as a Weights &amp; Biases artifact.</p> <p>Check this guide for preparing a dataset for registering in on Weights &amp; Biases</p> <ul> <li>Preparing the Dataset with Builder Script or TFDS Module.</li> </ul> <p>Usage:</p> <pre><code>import wandb\nfrom wandb_addons.dataset import upload_dataset\n# Initialize a W&amp;B Run\nwandb.init(project=\"my-awesome-project\", job_type=\"upload_dataset\")\n# Note that we should set our dataset name as the name of the artifact\nupload_dataset(dataset_name=\"my_awesome_dataset\", dataset_path=\"./my/dataset/path\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset. This name should follow the PEP8 package and module name convenmtions.</p> required <code>dataset_path</code> <code>str</code> <p>Path to the dataset.</p> required <code>aliases</code> <code>Optional[List[str]]</code> <p>Aliases to apply to this artifact. If the parameter <code>upload_tfrecords</code> is set to <code>True</code>, then the alias <code>\"tfrecord\"</code> is automatically appended to the provided list of aliases. Otherwise, the alias <code>\"tfds-module\"</code> is automatically appended to the provided list of aliases.</p> <code>None</code> <code>upload_tfrecords</code> <code>bool</code> <p>Upload dataset as TFRecords or not. If set to <code>False</code>, then the dataset is uploaded with a TFDS module, otherwise only the TFRrecords and the necessary metadata files would be uploaded.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>Whether to suppress the output of dataset build process or not.</p> <code>False</code> Source code in <code>wandb_addons/dataset/dataset_upload.py</code> <pre><code>def upload_dataset(\ndataset_name: str,\ndataset_path: str,\naliases: Optional[List[str]] = None,\nupload_tfrecords: bool = True,\nquiet: bool = False,\n):\n\"\"\"Upload and register a dataset with a TFDS module or a TFDS builder script as a\n    Weights &amp; Biases artifact. This function would verify if a TFDS build/registration is possible\n    with the current specified dataset path and upload it as a Weights &amp; Biases artifact.\n    !!! example \"Check this guide for preparing a dataset for registering in on Weights &amp; Biases\"\n        - [Preparing the Dataset with Builder Script or TFDS Module](../dataset_preparation).\n    **Usage:**\n    ```python\n    import wandb\n    from wandb_addons.dataset import upload_dataset\n    # Initialize a W&amp;B Run\n    wandb.init(project=\"my-awesome-project\", job_type=\"upload_dataset\")\n    # Note that we should set our dataset name as the name of the artifact\n    upload_dataset(dataset_name=\"my_awesome_dataset\", dataset_path=\"./my/dataset/path\")\n    ```\n    Args:\n        dataset_name (str): Name of the dataset. This name should follow the\n            [PEP8 package and module name convenmtions](https://peps.python.org/pep-0008/#package-and-module-names).\n        dataset_path (str): Path to the dataset.\n        aliases (Optional[List[str]]): Aliases to apply to this artifact. If the parameter `upload_tfrecords` is set\n            to `True`, then the alias `\"tfrecord\"` is automatically appended to the provided list of aliases.\n            Otherwise, the alias `\"tfds-module\"` is automatically appended to the provided list of aliases.\n        upload_tfrecords (bool): Upload dataset as TFRecords or not. If set to `False`, then the dataset is uploaded\n            with a TFDS module, otherwise only the TFRrecords and the necessary metadata files would be uploaded.\n        quiet (bool): Whether to suppress the output of dataset build process or not.\n    \"\"\"\nis_tfds_module_structure_valid = _verify_and_create_tfds_module_structure(\ndataset_name, dataset_path\n)\nif not is_tfds_module_structure_valid:\nwandb.termerror(\nf\"Unable to generate or detect valid TFDS module at {dataset_path}\"\n)\nelse:\nwandb.termlog(f\"Verified TFDS module at {dataset_path}\")\n_build_from_tfds_module(dataset_name, dataset_path, quiet=quiet)\nif upload_tfrecords:\n_upload_tfrecords(dataset_name, \"dataset\", aliases=aliases + [\"tfrecord\"])\nelse:\nupload_wandb_artifact(\nname=dataset_name,\nartifact_type=\"dataset\",\npath=dataset_path,\naliases=aliases + [\"tfds-module\"],\n)\n</code></pre>"},{"location":"dataset/dataset_loading/#wandb_addons.dataset.dataset_loading.load_dataset","title":"<code>load_dataset(artifact_address, artifact_type='dataset', remove_redundant_data_files=True, quiet=False)</code>","text":"<p>Load a dataset from a wandb artifact.</p> <p>Using this function you can load a dataset hosted as a wandb artifact in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model.</p> <p>Usage:</p> <pre><code>from wandb_addons.dataset import load_dataset\ndatasets, dataset_builder_info = load_dataset(\"geekyrakshit/artifact-accessor/monkey_species:v0\")\n</code></pre> <p>Example notebooks:</p> <ul> <li>\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>artifact_address</code> <code>str</code> <p>A human-readable name for the artifact, which is how you can identify the artifact in the UI or reference it in <code>use_artifact</code> calls. Names can contain letters, numbers, underscores, hyphens, and dots. The name must be unique across a project.</p> required <code>artifact_type</code> <code>str</code> <p>The type of the artifact, which is used to organize and differentiate artifacts. Common typesCinclude dataset or model, but you can use any string containing letters, numbers, underscores, hyphens, and dots.</p> <code>'dataset'</code> <code>remove_redundant_data_files</code> <code>bool</code> <p>Whether to remove the redundant data files from the artifacts directory after building the tfrecord dataset.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>Whether to suppress the output of dataset build process or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, tf.data.Dataset], DatasetInfo]</code> <p>A tuple of dictionary Dictionary mapping split aliases to the respective TensorFlow Prefetched dataset objects and the <code>tfds.core.DatasetInfo</code> that documents datasets, including its name, version, and features.</p> Source code in <code>wandb_addons/dataset/dataset_loading.py</code> <pre><code>def load_dataset(\nartifact_address: str,\nartifact_type: Optional[str] = \"dataset\",\nremove_redundant_data_files: bool = True,\nquiet: bool = False,\n) -&gt; Tuple[Dict[str, _DATASET_TYPE], DatasetInfo]:\n\"\"\"Load a dataset from a [wandb artifact](https://docs.wandb.ai/guides/artifacts).\n    Using this function you can load a dataset hosted as a\n    [wandb artifact](https://docs.wandb.ai/guides/artifacts) in a single line of code,\n    and use our powerful data processing methods to quickly get your dataset ready for\n    training in a deep learning model.\n    **Usage:**\n    ```python\n    from wandb_addons.dataset import load_dataset\n    datasets, dataset_builder_info = load_dataset(\"geekyrakshit/artifact-accessor/monkey_species:v0\")\n    ```\n    !!! example \"Example notebooks:\"\n        - [\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d](../examples/load_dataset).\n    Args:\n        artifact_address (str): A human-readable name for the artifact, which is how you can\n            identify the artifact in the UI or reference it in\n            [`use_artifact`](https://docs.wandb.ai/ref/python/run#use_artifact) calls. Names can\n            contain letters, numbers, underscores, hyphens, and dots. The name must be unique across\n            a project.\n        artifact_type (str): The type of the artifact, which is used to organize and differentiate\n            artifacts. Common typesCinclude dataset or model, but you can use any string containing\n            letters, numbers, underscores, hyphens, and dots.\n        remove_redundant_data_files (bool): Whether to remove the redundant data files from the\n            artifacts directory after building the tfrecord dataset.\n        quiet (bool): Whether to suppress the output of dataset build process or not.\n    Returns:\n        (Tuple[Dict[str, tf.data.Dataset], DatasetInfo]): A tuple of dictionary Dictionary mapping\n            split aliases to the respective\n            [TensorFlow Prefetched dataset](https://www.tensorflow.org/guide/data_performance#prefetching)\n            objects and the\n            [`tfds.core.DatasetInfo`](https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetInfo)\n            that documents datasets, including its name, version, and features.\n    \"\"\"\nartifact_dir = fetch_wandb_artifact(\nartifact_address=artifact_address, artifact_type=artifact_type\n)\nartifact_dir = _change_artifact_dir_name(artifact_dir)\ndataset_name = _get_dataset_name_from_artifact_address(artifact_address)\ntry:\ndataset_builder = tfds.builder_from_directory(artifact_dir)\ndataset_builder.download_and_prepare()\ndataset_splits, dataset_builder_info = _build_datasets(dataset_builder)\nexcept Exception as e:\nwandb.termwarn(\n\"Failed to detect TFRecords in the artifact. Attempting to build tfrecords\"\n)\ndataset_splits, dataset_builder_info = _load_dataset_from_tfds_module(\nartifact_address,\nartifact_dir,\ndataset_name,\nremove_redundant_data_files,\nquiet,\n)\nreturn dataset_splits, dataset_builder_info\n</code></pre>"},{"location":"dataset/dataset_preparation/","title":"Preparing the Dataset with Builder Script or TFDS Module","text":"<ol> <li> <p>For this example, we will be using the 10 Monkey Species dataset.</p> <p>The directory structure of this dataset is as follows: <pre><code>monkey_species_dataset\n|-- __init__.py\n|-- monkey_labels.txt\n|-- training\n|   `-- training\n|       |-- n0\n|       |-- n1\n|       |-- n2\n|       |-- n3\n|       |-- n4\n|       |-- n5\n|       |-- n6\n|       |-- n7\n|       |-- n8\n|       `-- n9\n|           |-- n9151jpg\n|           `-- n9160.png\n`-- validation\n    `-- validation\n        |-- n0\n        |-- n1\n        |-- n2\n        |-- n3\n        |-- n4\n        |-- n5\n        |-- n6\n        |-- n7\n        |-- n8\n        `-- n9\n</code></pre></p> </li> <li> <p>Next let us setup <code>wandb-addons</code>. We can do this using the following command:     <pre><code>git clone https://github.com/soumik12345/wandb-addons\npip install -q wandb-addons[dataset]\n</code></pre></p> <p>This would install <code>wandb-addons</code> and also optional dependencies including <code>tensorflow</code> and <code>tfds-nightly</code>, the nightly release of <code>tensorflow-datasets</code>.</p> </li> <li> <p>Now, let us <code>cd</code> into the <code>monkey_species_dataset</code> directory and initialize the tensorflow datasets template files, which would be used for interpreting and registering features from our dataset.     <pre><code>cd monkey_species_dataset\n# Create `monkey_species_dataset/monkey_species` template files.\ntfds new monkey_species </code></pre></p> <p>This would create a directory with the following structure inside the directory <code>monkey_species_dataset</code>:</p> <pre><code>monkey_species\n   |-- CITATIONS.bib\n   |-- README.md\n   |-- TAGS.txt\n   |-- __init__.py\n   |-- checksums.tsv\n   |-- dummy_data\n   |   `-- TODO-add_fake_data_in_this_directory.txt\n   |-- monkey_species_dataset_builder.py\n   `-- monkey_species_dataset_builder_test.py\n</code></pre> <p>The complete directory structure of <code>monkey_species_dataset</code> at this point is going to something like:</p> <pre><code>monkey_species_dataset\n|-- __init__.py\n|-- monkey_labels.txt\n|-- monkey_species\n|   |-- CITATIONS.bib\n|   |-- README.md\n|   |-- TAGS.txt\n|   |-- __init__.py\n|   |-- checksums.tsv\n|   |-- dummy_data\n|   |   `-- TODO-add_fake_data_in_this_directory.txt\n|   |-- monkey_species_dataset_builder.py\n|   `-- monkey_species_dataset_builder_test.py\n|-- training\n|   `-- training\n|       |-- n0\n|       |-- n1\n|       |-- n2\n|       |-- n3\n|       |-- n4\n|       |-- n5\n|       |-- n6\n|       |-- n7\n|       |-- n8\n|       `-- n9\n|           |-- n9151jpg\n|           `-- n9160.png\n`-- validation\n    `-- validation\n        |-- n0\n        |-- n1\n        |-- n2\n        |-- n3\n        |-- n4\n        |-- n5\n        |-- n6\n        |-- n7\n        |-- n8\n        `-- n9\n</code></pre> <p>Note</p> <p>The name with which you initialize the <code>tfds new</code> command would be used as the <code>name</code> of your dataset.</p> </li> <li> <p>Now we will write our dataset builder in the file <code>monkey_species_dataset/monkey_species/monkey_species_dataset_builder.py</code>. This logic for writing a dataset builder is exactly similar to that of creating the same for HuggingFace Datasets or a vanilla TensorFlow dataset.</p> <p>Note</p> <p>Alternative to step 3, you could also simply inclide a builder script <code>&lt;dataset_name&gt;.py</code> into the <code>monkey_species_dataset</code> directory, instead of creating the TFDS module.</p> <p>You can refer to the following examples</p> <ul> <li>An example of a dataset with a custom builder script</li> <li>An example of a dataset with a TFDS module</li> </ul> <p>You can refer to the following guides for writing builder scripts</p> <ul> <li>Writing custom datasets using <code>tfds</code>.</li> <li>HuggingFace: Create a dataset.</li> <li>HuggingFace: Create an Image Dataset.</li> </ul> </li> <li> <p>Now that our dataset is ready with the specifications for loading the features, we can upload it to our Weights &amp; Biases project as an artifact using the <code>upload_dataset</code> function, which would verify if the dataset build is successful or not before uploading the dataset.</p> <pre><code>import wandb\nfrom wandb_addons.dataset import upload_dataset\n# Initialize a W&amp;B Run\nwandb.init(project=\"my-awesome-project\", job_type=\"upload_dataset\") \n# Note that we should set our dataset name as the name of the artifact\nupload_dataset(name=\"my_awesome_dataset\", path=\"./my/dataset/path\", type=\"dataset\")\n</code></pre> <p>In order to load this dataset in your ML workflow you can simply use the <code>load_dataset</code> function:</p> <pre><code>from wandb_addons.dataset import load_dataset\ndatasets, dataset_builder_info = load_dataset(\"entity-name/my-awesome-project/my_awesome_dataset:v0\")\n</code></pre> <p>Note</p> <ul> <li>In the <code>upload_dataset</code> function by default convert a registered dataset to TFRecords (like this artifact). You can alternatively upload the dataset in its original state along with the added TFDS module containing the builder script by simply setting <code>upload_tfrecords</code> parameter to <code>False</code>.</li> <li>Note that this won't affect loading the dataset using <code>load_dataset</code>, dataset loading from artifacts would work as long as the artifact contains either the TFRecords or the original dataset with the TFDS module.</li> <li>The TFRecord artifact has to follow the specification specified in this guide. However, if you're using the <code>upload_dataset</code> function, you don't need to worry about this.</li> </ul> <p>You can take a look at this artifact that demonstrates the aforementioned directory structure and builder logic.</p> </li> </ol>"},{"location":"dataset/examples/load_dataset/","title":"\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom wandb_addons.dataset import load_dataset\n</pre> import tensorflow as tf import matplotlib.pyplot as plt from wandb_addons.dataset import load_dataset <pre>2023-04-30 12:34:52.474435: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> <p>Just pass in the artifact address of the dataset artifact, and we are all set. For detailed documentation of all parameters and option, refer to this page.</p> <p>Note: For loading and ingesting the dataset from a wandb artifact, its not compulsory to initialize a run. However, loading inside the context of a run has added advantages of tracking lineage of artifacts and ease of versioning.</p> In\u00a0[2]: Copied! <pre>datasets, dataset_builder_info = load_dataset(\n    \"geekyrakshit/artifact-accessor/monkey_dataset:v1\", quiet=True\n)\n</pre> datasets, dataset_builder_info = load_dataset(     \"geekyrakshit/artifact-accessor/monkey_dataset:v1\", quiet=True ) <pre>wandb: Downloading large artifact monkey_dataset:v1, 553.56MB. 8 files... \nwandb:   8 of 8 files downloaded.  \nDone. 0:0:0.9\nwandb: Building dataset for split: train...\n2023-04-30 12:34:56.083939: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nwandb: Built dataset for split: train, num_shards: 4, num_examples: 1096\nwandb: Building dataset for split: val...\nwandb: Built dataset for split: val, num_shards: 1, num_examples: 272\n</pre> <p>Now we that we have created the TensorFlow datasets corresponding to the splits along with the general info of the dataset, we can verify them.</p> In\u00a0[3]: Copied! <pre>class_names = dataset_builder_info.features[\"label\"].names\n</pre> class_names = dataset_builder_info.features[\"label\"].names In\u00a0[4]: Copied! <pre>sample = next(iter(datasets[\"train\"]))\nplt.imshow(sample[\"image\"].numpy())\nlabel_name = class_names[sample[\"label\"].numpy()]\nplt.title(f\"Label: {label_name}\")\nplt.show()\n</pre> sample = next(iter(datasets[\"train\"])) plt.imshow(sample[\"image\"].numpy()) label_name = class_names[sample[\"label\"].numpy()] plt.title(f\"Label: {label_name}\") plt.show() In\u00a0[5]: Copied! <pre>sample = next(iter(datasets[\"val\"]))\nplt.imshow(sample[\"image\"].numpy())\nlabel_name = class_names[sample[\"label\"].numpy()]\nplt.title(f\"Label: {label_name}\")\nplt.show()\n</pre> sample = next(iter(datasets[\"val\"])) plt.imshow(sample[\"image\"].numpy()) label_name = class_names[sample[\"label\"].numpy()] plt.title(f\"Label: {label_name}\") plt.show() In\u00a0[6]: Copied! <pre>print(\"Train Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"train\"]).numpy())\nprint(\"Validation Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"val\"]).numpy())\n</pre> print(\"Train Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"train\"]).numpy()) print(\"Validation Dataset Cardinality:\", tf.data.experimental.cardinality(datasets[\"val\"]).numpy()) <pre>Train Dataset Cardinality: 1096\nValidation Dataset Cardinality: 272\n</pre> <p>Now that we have verified the dataset splits, we can use them to build high-performance input pipelines for our training workflows not onlt in TensorFlow, but also JAX and PyTorch. You can refer to the following docs regarding building input pipelines:</p> <ul> <li><code>tf.data</code>: Build TensorFlow input pipelines.</li> <li>Better performance with the <code>tf.data</code> API.</li> <li>TFDS for Jax and PyTorch</li> </ul>"},{"location":"dataset/examples/load_dataset/#data-loading-with-wandb-artifacts","title":"\ud83d\udd25 Data Loading with WandB Artifacts \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook demonstrates the usage of a simple and easy-to use data loading API built on top of Tensorflow Datasets and WandB Artifacts.</p>"},{"location":"dataset/examples/load_dataset/#loading-the-dataset","title":"Loading the Dataset\u00b6","text":"<p>Now that the dataset is uploaded as an artifact with the builder logic, loading and ingesting the dataset is incredibly easy. To do this we would simply use the <code>wandb_addons.dataset.load_dataset</code> function.</p>"},{"location":"monai/monai/","title":"MonAI Handlers","text":"<p>Handlers for experiment tracking on Weights &amp; Biases with MonAI Engines.</p>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler","title":"<code>WandbStatsHandler</code>","text":"<p><code>WandbStatsHandler</code> defines a set of Ignite Event-handlers for all the Weights &amp; Biases logging logic. It can be used for any Ignite Engine(trainer, validator and evaluator) and support both epoch level and iteration level. The expected data source is Ignite <code>engine.state.output</code> and <code>engine.state.metrics</code>.</p> Default behaviors <ul> <li>When EPOCH_COMPLETED, write each dictionary item in <code>engine.state.metrics</code> to     Weights &amp; Biases.</li> <li>When ITERATION_COMPLETED, write each dictionary item in     <code>self.output_transform(engine.state.output)</code> to Weights &amp; Biases.</li> </ul> <p>Usage:</p> <pre><code># WandbStatsHandler for logging training metrics and losses at\n# every iteration to Weights &amp; Biases\ntrain_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\ntrain_wandb_stats_handler.attach(trainer)\n# WandbStatsHandler for logging validation metrics and losses at\n# every iteration to Weights &amp; Biases\nval_wandb_stats_handler = WandbStatsHandler(\noutput_transform=lambda x: None,\nglobal_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_wandb_stats_handler.attach(evaluator)\n</code></pre> Example notebooks: <ul> <li>3D classification using MonAI.</li> <li>3D segmentation using MonAI.</li> </ul> Pull Request to add <code>WandbStatsHandler</code> to MonAI repository <p>There is an open pull request to add <code>WandbStatsHandler</code> to MonAI.</p> <p>Parameters:</p> Name Type Description Default <code>iteration_log</code> <code>bool</code> <p>Whether to write data to Weights &amp; Biases when iteration completed, default to <code>True</code>.</p> <code>True</code> <code>epoch_log</code> <code>bool</code> <p>Whether to write data to Weights &amp; Biases when epoch completed, default to <code>True</code>.</p> <code>True</code> <code>epoch_event_writer</code> <code>Optional[Callable[[Engine, Any], Any]]</code> <p>Customized callable Weights &amp; Biases writer for epoch level. Must accept parameter \"engine\" and \"summary_writer\", use default event writer if None.</p> <code>None</code> <code>epoch_interval</code> <code>int</code> <p>The epoch interval at which the epoch_event_writer is called. Defaults to 1.</p> <code>1</code> <code>iteration_event_writer</code> <code>Optional[Callable[[Engine, Any], Any]]</code> <p>Customized callable Weights &amp; Biases writer for iteration level. Must accept parameter \"engine\" and \"summary_writer\", use default event writer if None.</p> <code>None</code> <code>iteration_interval</code> <code>int</code> <p>The iteration interval at which the iteration_event_writer is called. Defaults to 1.</p> <code>1</code> <code>output_transform</code> <code>Callable</code> <p>A callable that is used to transform the <code>ignite.engine.state.output</code> into a scalar to plot, or a dictionary of <code>{key: scalar}</code>. In the latter case, the output string will be formatted as key: value. By default this value plotting happens when every iteration completed. The default behavior is to print loss from output[0] as output is a decollated list and we replicated loss value for every item of the decollated list. <code>engine.state</code> and <code>output_transform</code> inherit from the ignite concept: https://pytorch.org/ignite/concepts.html#state, explanation and usage example are in the tutorial: https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.</p> <code>lambda x: x[0]</code> <code>global_epoch_transform</code> <code>Callable</code> <p>A callable that is used to customize global epoch number. For example, in evaluation, the evaluator engine might want to use trainer engines epoch number when plotting epoch vs metric curves.</p> <code>lambda x: x</code> <code>state_attributes</code> <code>Optional[Sequence[str]]</code> <p>Expected attributes from <code>engine.state</code>, if provided, will extract them when epoch completed.</p> <code>None</code> <code>tag_name</code> <code>str</code> <p>When iteration output is a scalar, tag_name is used to plot, defaults to <code>'Loss'</code>.</p> <code>DEFAULT_TAG</code> Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>class WandbStatsHandler:\n\"\"\"\n    `WandbStatsHandler` defines a set of Ignite Event-handlers for all the Weights &amp; Biases logging\n    logic. It can be used for any Ignite Engine(trainer, validator and evaluator) and support both\n    epoch level and iteration level. The expected data source is Ignite `engine.state.output` and\n    `engine.state.metrics`.\n    Default behaviors:\n        - When EPOCH_COMPLETED, write each dictionary item in `engine.state.metrics` to\n            Weights &amp; Biases.\n        - When ITERATION_COMPLETED, write each dictionary item in\n            `self.output_transform(engine.state.output)` to Weights &amp; Biases.\n    **Usage:**\n    ```python\n    # WandbStatsHandler for logging training metrics and losses at\n    # every iteration to Weights &amp; Biases\n    train_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\n    train_wandb_stats_handler.attach(trainer)\n    # WandbStatsHandler for logging validation metrics and losses at\n    # every iteration to Weights &amp; Biases\n    val_wandb_stats_handler = WandbStatsHandler(\n        output_transform=lambda x: None,\n        global_epoch_transform=lambda x: trainer.state.epoch,\n    )\n    val_wandb_stats_handler.attach(evaluator)\n    ```\n    ??? example \"Example notebooks:\"\n        - [3D classification using MonAI](../examples/densenet_training_dict).\n        - [3D segmentation using MonAI](../examples/unet_3d_segmentation).\n    ??? note \"Pull Request to add `WandbStatsHandler` to MonAI repository\"\n        There is an [open pull request](https://github.com/Project-MONAI/MONAI/pull/6305)\n        to add `WandbStatsHandler` to [MonAI](https://github.com/Project-MONAI/MONAI).\n    Args:\n        iteration_log (bool): Whether to write data to Weights &amp; Biases when iteration completed,\n            default to `True`.\n        epoch_log (bool): Whether to write data to Weights &amp; Biases when epoch completed, default to\n            `True`.\n        epoch_event_writer (Optional[Callable[[Engine, Any], Any]]): Customized callable\n            Weights &amp; Biases writer for epoch level. Must accept parameter \"engine\" and\n            \"summary_writer\", use default event writer if None.\n        epoch_interval (int): The epoch interval at which the epoch_event_writer is called. Defaults\n            to 1.\n        iteration_event_writer (Optional[Callable[[Engine, Any], Any]]): Customized callable\n            Weights &amp; Biases writer for iteration level. Must accept parameter \"engine\" and\n            \"summary_writer\", use default event writer if None.\n        iteration_interval (int): The iteration interval at which the iteration_event_writer is\n            called. Defaults to 1.\n        output_transform (Callable): A callable that is used to transform the\n            `ignite.engine.state.output` into a scalar to plot, or a dictionary of `{key: scalar}`. In\n            the latter case, the output string will be formatted as key: value. By default this value\n            plotting happens when every iteration completed. The default behavior is to print loss\n            from output[0] as output is a decollated list and we replicated loss value for every item\n            of the decollated list. `engine.state` and `output_transform` inherit from the ignite\n            concept: https://pytorch.org/ignite/concepts.html#state, explanation and usage example are\n            in the tutorial:\n            https://github.com/Project-MONAI/tutorials/blob/master/modules/batch_output_transform.ipynb.\n        global_epoch_transform (Callable): A callable that is used to customize global epoch number. For\n            example, in evaluation, the evaluator engine might want to use trainer engines epoch number\n            when plotting epoch vs metric curves.\n        state_attributes (Optional[Sequence[str]]): Expected attributes from `engine.state`, if provided,\n            will extract them when epoch completed.\n        tag_name (str): When iteration output is a scalar, tag_name is used to plot, defaults to `'Loss'`.\n    \"\"\"\ndef __init__(\nself,\niteration_log: bool = True,\nepoch_log: bool = True,\nepoch_event_writer: Optional[Callable[[Engine, Any], Any]] = None,\nepoch_interval: int = 1,\niteration_event_writer: Optional[Callable[[Engine, Any], Any]] = None,\niteration_interval: int = 1,\noutput_transform: Callable = lambda x: x[0],\nglobal_epoch_transform: Callable = lambda x: x,\nstate_attributes: Optional[Sequence[str]] = None,\ntag_name: str = DEFAULT_TAG,\n):\nif wandb.run is None:\nraise wandb.Error(\"You must call `wandb.init()` before WandbStatsHandler()\")\nself.iteration_log = iteration_log\nself.epoch_log = epoch_log\nself.epoch_event_writer = epoch_event_writer\nself.epoch_interval = epoch_interval\nself.iteration_event_writer = iteration_event_writer\nself.iteration_interval = iteration_interval\nself.output_transform = output_transform\nself.global_epoch_transform = global_epoch_transform\nself.state_attributes = state_attributes\nself.tag_name = tag_name\ndef attach(self, engine: Engine) -&gt; None:\n\"\"\"\n        Register a set of Ignite Event-Handlers to a specified Ignite engine.\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\nif self.iteration_log and not engine.has_event_handler(\nself.iteration_completed, Events.ITERATION_COMPLETED\n):\nengine.add_event_handler(\nEvents.ITERATION_COMPLETED(every=self.iteration_interval),\nself.iteration_completed,\n)\nif self.epoch_log and not engine.has_event_handler(\nself.epoch_completed, Events.EPOCH_COMPLETED\n):\nengine.add_event_handler(\nEvents.EPOCH_COMPLETED(every=self.epoch_interval), self.epoch_completed\n)\ndef epoch_completed(self, engine: Engine) -&gt; None:\n\"\"\"\n        Handler for train or validation/evaluation epoch completed Event. Write epoch level events\n        to Weights &amp; Biases, default values are from Ignite `engine.state.metrics` dict.\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\nif self.epoch_event_writer is not None:\nself.epoch_event_writer(engine)\nelse:\nself._default_epoch_writer(engine)\ndef iteration_completed(self, engine: Engine) -&gt; None:\n\"\"\"\n        Handler for train or validation/evaluation iteration completed Event. Write iteration level\n        events to Weighs &amp; Biases, default values are from Ignite `engine.state.output`.\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\nif self.iteration_event_writer is not None:\nself.iteration_event_writer(engine)\nelse:\nself._default_iteration_writer(engine)\ndef _default_epoch_writer(self, engine: Engine) -&gt; None:\n\"\"\"\n        Execute epoch level event write operation. Default to write the values from Ignite\n        `engine.state.metrics` dict and write the values of specified attributes of `engine.state`\n        to [Weights &amp; Biases](https://wandb.ai/site).\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\nsummary_dict = engine.state.metrics\nfor key, value in summary_dict.items():\nif is_scalar(value):\nvalue = value.item() if isinstance(value, torch.Tensor) else value\nwandb.log({key: value})\nif self.state_attributes is not None:\nfor attr in self.state_attributes:\nvalue = getattr(engine.state, attr, None)\nvalue = value.item() if isinstance(value, torch.Tensor) else value\nwandb.log({attr: value})\ndef _default_iteration_writer(self, engine: Engine) -&gt; None:\n\"\"\"\n        Execute iteration level event write operation based on Ignite `engine.state.output` data.\n        Extract the values from `self.output_transform(engine.state.output)`. Since\n        `engine.state.output` is a decollated list and we replicated the loss value for every item\n        of the decollated list, the default behavior is to track the loss from `output[0]`.\n        Args:\n            engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n                or evaluator.\n        \"\"\"\nloss = self.output_transform(engine.state.output)\nif loss is None:\nreturn  # do nothing if output is empty\nlog_dict = dict()\nif isinstance(loss, dict):\nfor key, value in loss.items():\nif not is_scalar(value):\nwarnings.warn(\n\"ignoring non-scalar output in WandbStatsHandler,\"\n\" make sure `output_transform(engine.state.output)` returns\"\n\" a scalar or dictionary of key and scalar pairs to avoid this warning.\"\n\" {}:{}\".format(key, type(value))\n)\ncontinue  # not plot multi dimensional output\nlog_dict[key] = (\nvalue.item() if isinstance(value, torch.Tensor) else value\n)\nelif is_scalar(loss):  # not printing multi dimensional output\nlog_dict[self.tag_name] = (\nloss.item() if isinstance(loss, torch.Tensor) else loss\n)\nelse:\nwarnings.warn(\n\"ignoring non-scalar output in WandbStatsHandler,\"\n\" make sure `output_transform(engine.state.output)` returns\"\n\" a scalar or a dictionary of key and scalar pairs to avoid this warning.\"\n\" {}\".format(type(loss))\n)\nwandb.log(log_dict)\ndef close(self):\n\"\"\"Close `WandbStatsHandler`\"\"\"\nwandb.finish()\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.attach","title":"<code>attach(engine)</code>","text":"<p>Register a set of Ignite Event-Handlers to a specified Ignite engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>ignite.engine.engine.Engine</code> <p>Ignite Engine, it can be a trainer, validator or evaluator.</p> required Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def attach(self, engine: Engine) -&gt; None:\n\"\"\"\n    Register a set of Ignite Event-Handlers to a specified Ignite engine.\n    Args:\n        engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n            or evaluator.\n    \"\"\"\nif self.iteration_log and not engine.has_event_handler(\nself.iteration_completed, Events.ITERATION_COMPLETED\n):\nengine.add_event_handler(\nEvents.ITERATION_COMPLETED(every=self.iteration_interval),\nself.iteration_completed,\n)\nif self.epoch_log and not engine.has_event_handler(\nself.epoch_completed, Events.EPOCH_COMPLETED\n):\nengine.add_event_handler(\nEvents.EPOCH_COMPLETED(every=self.epoch_interval), self.epoch_completed\n)\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.close","title":"<code>close()</code>","text":"<p>Close <code>WandbStatsHandler</code></p> Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def close(self):\n\"\"\"Close `WandbStatsHandler`\"\"\"\nwandb.finish()\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.epoch_completed","title":"<code>epoch_completed(engine)</code>","text":"<p>Handler for train or validation/evaluation epoch completed Event. Write epoch level events to Weights &amp; Biases, default values are from Ignite <code>engine.state.metrics</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>ignite.engine.engine.Engine</code> <p>Ignite Engine, it can be a trainer, validator or evaluator.</p> required Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def epoch_completed(self, engine: Engine) -&gt; None:\n\"\"\"\n    Handler for train or validation/evaluation epoch completed Event. Write epoch level events\n    to Weights &amp; Biases, default values are from Ignite `engine.state.metrics` dict.\n    Args:\n        engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n            or evaluator.\n    \"\"\"\nif self.epoch_event_writer is not None:\nself.epoch_event_writer(engine)\nelse:\nself._default_epoch_writer(engine)\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.stats_handler.WandbStatsHandler.iteration_completed","title":"<code>iteration_completed(engine)</code>","text":"<p>Handler for train or validation/evaluation iteration completed Event. Write iteration level events to Weighs &amp; Biases, default values are from Ignite <code>engine.state.output</code>.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>ignite.engine.engine.Engine</code> <p>Ignite Engine, it can be a trainer, validator or evaluator.</p> required Source code in <code>wandb_addons/monai/stats_handler.py</code> <pre><code>def iteration_completed(self, engine: Engine) -&gt; None:\n\"\"\"\n    Handler for train or validation/evaluation iteration completed Event. Write iteration level\n    events to Weighs &amp; Biases, default values are from Ignite `engine.state.output`.\n    Args:\n        engine (ignite.engine.engine.Engine): Ignite Engine, it can be a trainer, validator\n            or evaluator.\n    \"\"\"\nif self.iteration_event_writer is not None:\nself.iteration_event_writer(engine)\nelse:\nself._default_iteration_writer(engine)\n</code></pre>"},{"location":"monai/monai/#wandb_addons.monai.checkpoint_handler.WandbModelCheckpointSaver","title":"<code>WandbModelCheckpointSaver</code>","text":"<p>         Bases: <code>BaseSaveHandler</code></p> <p><code>WandbModelCheckpointSaver</code> is a save handler for PyTorch Ignite that saves model checkpoints as Weights &amp; Biases Artifacts.</p> <p>Usage:</p> <pre><code>from wandb_addons.monai import WandbModelCheckpointSaver\ncheckpoint_handler = Checkpoint(\n{\"model\": model, \"optimizer\": optimizer},\nWandbModelCheckpointSaver(),\nn_saved=1,\nfilename_prefix=\"best_checkpoint\",\nscore_name=metric_name,\nglobal_step_transform=global_step_from_engine(trainer)\n)\nevaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n</code></pre> Source code in <code>wandb_addons/monai/checkpoint_handler.py</code> <pre><code>class WandbModelCheckpointSaver(BaseSaveHandler):\n\"\"\"`WandbModelCheckpointSaver` is a save handler for PyTorch Ignite that saves model checkpoints as\n    [Weights &amp; Biases Artifacts](https://docs.wandb.ai/guides/artifacts).\n    Usage:\n    ```python\n    from wandb_addons.monai import WandbModelCheckpointSaver\n    checkpoint_handler = Checkpoint(\n        {\"model\": model, \"optimizer\": optimizer},\n        WandbModelCheckpointSaver(),\n        n_saved=1,\n        filename_prefix=\"best_checkpoint\",\n        score_name=metric_name,\n        global_step_transform=global_step_from_engine(trainer)\n    )\n    evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n    ```\n    \"\"\"\n@one_rank_only()\ndef __init__(self):\nif wandb.run is None:\nraise wandb.Error(\n\"You must call `wandb.init()` before `WandbModelCheckpointSaver()`\"\n)\nself.checkpoint_dir = tempfile.mkdtemp()\n@one_rank_only()\ndef __call__(self, checkpoint: Mapping, filename: Union[str, Path]):\ncheckpoint_path = os.path.join(self.checkpoint_dir, filename)\ntorch.save(checkpoint, checkpoint_path)\nartifact = wandb.Artifact(f\"{wandb.run.id}-checkpoint\", type=\"model\")\nif os.path.isfile(checkpoint_path):\nartifact.add_file(checkpoint_path)\nelif os.path.isdir(checkpoint_path):\nartifact.add_dir(checkpoint_path)\nelse:\nraise wandb.Error(\nf\"Unable to local checkpoint path {checkpoint_path} to artifact\"\n)\nwandb.log_artifact(artifact)\n@one_rank_only()\ndef remove(self, filename):\nif os.path.exists(filename):\nshutil.rmtree(filename)\n</code></pre>"},{"location":"monai/examples/densenet_training_dict/","title":"Densenet training dict","text":"<p>Original Source: https://github.com/Project-MONAI/tutorials/blob/main/3d_classification/ignite/densenet_training_dict.py</p> In\u00a0[\u00a0]: Copied! <pre>!mkdir dataset\n%cd dataset\n!wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar\n!wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T2.tar\n!tar -xf IXI-T1.tar &amp;&amp; tar -xf IXI-T2.tar &amp;&amp; rm -rf IXI-T1.tar &amp;&amp; rm -rf IXI-T2.tar\n%cd ..\n!git clone https://github.com/soumik12345/wandb-addons\n!pip install -q --upgrade pip setuptools\n!pip install -q -e wandb-addons[monai]\n</pre> !mkdir dataset %cd dataset !wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar !wget http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T2.tar !tar -xf IXI-T1.tar &amp;&amp; tar -xf IXI-T2.tar &amp;&amp; rm -rf IXI-T1.tar &amp;&amp; rm -rf IXI-T2.tar %cd .. !git clone https://github.com/soumik12345/wandb-addons !pip install -q --upgrade pip setuptools !pip install -q -e wandb-addons[monai] In\u00a0[\u00a0]: Copied! <pre>import os\nimport sys\nfrom glob import glob\n\nimport numpy as np\nimport wandb\nimport torch\nfrom ignite.engine import Events, _prepare_batch, create_supervised_evaluator, create_supervised_trainer\nfrom ignite.handlers import EarlyStopping, ModelCheckpoint\n\nimport monai\nfrom monai.data import decollate_batch, DataLoader\nfrom monai.handlers import ROCAUC, StatsHandler, TensorBoardStatsHandler, stopping_fn_from_metric\nfrom monai.transforms import Activations, AsDiscrete, Compose, LoadImaged, RandRotate90d, Resized, ScaleIntensityd\n\nfrom wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler\n\nmonai.config.print_config()\n</pre> import os import sys from glob import glob  import numpy as np import wandb import torch from ignite.engine import Events, _prepare_batch, create_supervised_evaluator, create_supervised_trainer from ignite.handlers import EarlyStopping, ModelCheckpoint  import monai from monai.data import decollate_batch, DataLoader from monai.handlers import ROCAUC, StatsHandler, TensorBoardStatsHandler, stopping_fn_from_metric from monai.transforms import Activations, AsDiscrete, Compose, LoadImaged, RandRotate90d, Resized, ScaleIntensityd  from wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler  monai.config.print_config() In\u00a0[\u00a0]: Copied! <pre>wandb.tensorboard.patch(root_logdir=\"./runs\")\nwandb.init(project=\"monai-integration\", sync_tensorboard=True, save_code=True)\n</pre> wandb.tensorboard.patch(root_logdir=\"./runs\") wandb.init(project=\"monai-integration\", sync_tensorboard=True, save_code=True) In\u00a0[\u00a0]: Copied! <pre>images = glob(\"./dataset/*\")[:20]\nlabels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64)\ntrain_files = [{\"img\": img, \"label\": label} for img, label in zip(images[:10], labels[:10])]\nval_files = [{\"img\": img, \"label\": label} for img, label in zip(images[-10:], labels[-10:])]\n</pre> images = glob(\"./dataset/*\")[:20] labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64) train_files = [{\"img\": img, \"label\": label} for img, label in zip(images[:10], labels[:10])] val_files = [{\"img\": img, \"label\": label} for img, label in zip(images[-10:], labels[-10:])] In\u00a0[\u00a0]: Copied! <pre>train_transforms = Compose(\n    [\n        LoadImaged(keys=[\"img\"], ensure_channel_first=True),\n        ScaleIntensityd(keys=[\"img\"]),\n        Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n        RandRotate90d(keys=[\"img\"], prob=0.8, spatial_axes=[0, 2]),\n    ]\n)\nval_transforms = Compose(\n    [\n        LoadImaged(keys=[\"img\"], ensure_channel_first=True),\n        ScaleIntensityd(keys=[\"img\"]),\n        Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),\n    ]\n)\n</pre> train_transforms = Compose(     [         LoadImaged(keys=[\"img\"], ensure_channel_first=True),         ScaleIntensityd(keys=[\"img\"]),         Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),         RandRotate90d(keys=[\"img\"], prob=0.8, spatial_axes=[0, 2]),     ] ) val_transforms = Compose(     [         LoadImaged(keys=[\"img\"], ensure_channel_first=True),         ScaleIntensityd(keys=[\"img\"]),         Resized(keys=[\"img\"], spatial_size=(96, 96, 96)),     ] ) In\u00a0[\u00a0]: Copied! <pre>check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\ncheck_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\ncheck_data = monai.utils.misc.first(check_loader)\nprint(check_data[\"img\"].shape, check_data[\"label\"])\n</pre> check_ds = monai.data.Dataset(data=train_files, transform=train_transforms) check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available()) check_data = monai.utils.misc.first(check_loader) print(check_data[\"img\"].shape, check_data[\"label\"]) In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device)\nloss = torch.nn.CrossEntropyLoss()\nlr = 1e-5\nopt = torch.optim.Adam(net.parameters(), lr)\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=2).to(device) loss = torch.nn.CrossEntropyLoss() lr = 1e-5 opt = torch.optim.Adam(net.parameters(), lr) In\u00a0[\u00a0]: Copied! <pre>def prepare_batch(batch, device=None, non_blocking=False):\n    return _prepare_batch((batch[\"img\"], batch[\"label\"]), device, non_blocking)\n\ntrainer = create_supervised_trainer(net, opt, loss, device, False, prepare_batch=prepare_batch)\n</pre> def prepare_batch(batch, device=None, non_blocking=False):     return _prepare_batch((batch[\"img\"], batch[\"label\"]), device, non_blocking)  trainer = create_supervised_trainer(net, opt, loss, device, False, prepare_batch=prepare_batch) In\u00a0[\u00a0]: Copied! <pre>checkpoint_handler = WandbModelCheckpointHandler(\"./runs_dict/\", \"net\", n_saved=10, require_empty=False)\ntrainer.add_event_handler(\n    event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={\"net\": net, \"opt\": opt}\n)\n\ntrain_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x)\ntrain_stats_handler.attach(trainer)\n\ntrain_tensorboard_stats_handler = TensorBoardStatsHandler(output_transform=lambda x: x)\ntrain_tensorboard_stats_handler.attach(trainer)\n\n# WandbStatsHandler logs loss at every iteration\ntrain_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\ntrain_wandb_stats_handler.attach(trainer)\n</pre> checkpoint_handler = WandbModelCheckpointHandler(\"./runs_dict/\", \"net\", n_saved=10, require_empty=False) trainer.add_event_handler(     event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={\"net\": net, \"opt\": opt} )  train_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x) train_stats_handler.attach(trainer)  train_tensorboard_stats_handler = TensorBoardStatsHandler(output_transform=lambda x: x) train_tensorboard_stats_handler.attach(trainer)  # WandbStatsHandler logs loss at every iteration train_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x) train_wandb_stats_handler.attach(trainer) In\u00a0[\u00a0]: Copied! <pre># set parameters for validation\nvalidation_every_n_epochs = 1\n\nmetric_name = \"AUC\"\n# add evaluation metric to the evaluator engine\nval_metrics = {metric_name: ROCAUC()}\n\npost_label = Compose([AsDiscrete(to_onehot=2)])\npost_pred = Compose([Activations(softmax=True)])\n# Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration,\n# user can add output_transform to return other values\nevaluator = create_supervised_evaluator(\n    net,\n    val_metrics,\n    device,\n    True,\n    prepare_batch=prepare_batch,\n    output_transform=lambda x, y, y_pred: (\n        [post_pred(i) for i in decollate_batch(y_pred)],\n        [post_label(i) for i in decollate_batch(y, detach=False)],\n    ),\n)\n</pre>  # set parameters for validation validation_every_n_epochs = 1  metric_name = \"AUC\" # add evaluation metric to the evaluator engine val_metrics = {metric_name: ROCAUC()}  post_label = Compose([AsDiscrete(to_onehot=2)]) post_pred = Compose([Activations(softmax=True)]) # Ignite evaluator expects batch=(img, label) and returns output=(y_pred, y) at every iteration, # user can add output_transform to return other values evaluator = create_supervised_evaluator(     net,     val_metrics,     device,     True,     prepare_batch=prepare_batch,     output_transform=lambda x, y, y_pred: (         [post_pred(i) for i in decollate_batch(y_pred)],         [post_label(i) for i in decollate_batch(y, detach=False)],     ), ) In\u00a0[\u00a0]: Copied! <pre># add stats event handler to print validation stats via evaluator\nval_stats_handler = StatsHandler(\n    name=\"evaluator\",\n    output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)  # fetch global epoch number from trainer\nval_stats_handler.attach(evaluator)\n\n# add handler to record metrics to TensorBoard at every epoch\nval_tensorboard_stats_handler = TensorBoardStatsHandler(\n    output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)  # fetch global epoch number from trainer\nval_tensorboard_stats_handler.attach(evaluator)\n\n# add handler to record metrics to Weights &amp; Biases at every epoch\nval_wandb_stats_handler = WandbStatsHandler(\n    output_transform=lambda x: None,\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_wandb_stats_handler.attach(trainer)\n</pre> # add stats event handler to print validation stats via evaluator val_stats_handler = StatsHandler(     name=\"evaluator\",     output_transform=lambda x: None,  # no need to print loss value, so disable per iteration output     global_epoch_transform=lambda x: trainer.state.epoch, )  # fetch global epoch number from trainer val_stats_handler.attach(evaluator)  # add handler to record metrics to TensorBoard at every epoch val_tensorboard_stats_handler = TensorBoardStatsHandler(     output_transform=lambda x: None,  # no need to plot loss value, so disable per iteration output     global_epoch_transform=lambda x: trainer.state.epoch, )  # fetch global epoch number from trainer val_tensorboard_stats_handler.attach(evaluator)  # add handler to record metrics to Weights &amp; Biases at every epoch val_wandb_stats_handler = WandbStatsHandler(     output_transform=lambda x: None,     global_epoch_transform=lambda x: trainer.state.epoch, ) val_wandb_stats_handler.attach(trainer) In\u00a0[\u00a0]: Copied! <pre># add early stopping handler to evaluator\nearly_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer)\nevaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper)\n</pre> # add early stopping handler to evaluator early_stopper = EarlyStopping(patience=4, score_function=stopping_fn_from_metric(metric_name), trainer=trainer) evaluator.add_event_handler(event_name=Events.EPOCH_COMPLETED, handler=early_stopper) In\u00a0[\u00a0]: Copied! <pre># create a validation data loader\nval_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\nval_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n</pre> # create a validation data loader val_ds = monai.data.Dataset(data=val_files, transform=val_transforms) val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available()) In\u00a0[\u00a0]: Copied! <pre>@trainer.on(Events.EPOCH_COMPLETED(every=validation_every_n_epochs))\ndef run_validation(engine):\n    evaluator.run(val_loader)\n</pre> @trainer.on(Events.EPOCH_COMPLETED(every=validation_every_n_epochs)) def run_validation(engine):     evaluator.run(val_loader) In\u00a0[\u00a0]: Copied! <pre># create a training data loader\ntrain_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\ntrain_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n</pre> # create a training data loader train_ds = monai.data.Dataset(data=train_files, transform=train_transforms) train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available()) In\u00a0[\u00a0]: Copied! <pre>train_epochs = 30\nstate = trainer.run(train_loader, train_epochs)\nprint(state)\nwandb.finish()\n</pre> train_epochs = 30 state = trainer.run(train_loader, train_epochs) print(state) wandb.finish() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"monai/examples/unet_3d_segmentation/","title":"Unet 3d segmentation","text":"<p>Original Source: https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/unet_segmentation_3d_ignite.ipynb</p> In\u00a0[\u00a0]: Copied! <pre>!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n!python -c \"import wandb\" || pip install -q wandb\n!pip install -q --upgrade git+https://github.com/soumik12345/wandb-addons@\n</pre> !python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\" !python -c \"import wandb\" || pip install -q wandb !pip install -q --upgrade git+https://github.com/soumik12345/wandb-addons@ In\u00a0[\u00a0]: Copied! <pre>import glob\nimport logging\nimport os\nfrom pathlib import Path\nimport shutil\nimport sys\nimport tempfile\n\nimport nibabel as nib\nimport numpy as np\nfrom monai.config import print_config\nfrom monai.data import (\n    ArrayDataset,\n    create_test_image_3d,\n    decollate_batch,\n    DataLoader\n)\nfrom monai.handlers import (\n    MeanDice,\n    StatsHandler,\n    TensorBoardImageHandler,\n    TensorBoardStatsHandler,\n)\nfrom monai.losses import DiceLoss\nfrom monai.networks.nets import UNet\nfrom monai.transforms import (\n    Activations,\n    EnsureChannelFirst,\n    AsDiscrete,\n    Compose,\n    LoadImage,\n    RandSpatialCrop,\n    Resize,\n    ScaleIntensity,\n)\nfrom monai.utils import first\n\nimport wandb\nfrom wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler\n\nimport ignite\nimport torch\nfrom tqdm.auto import tqdm\n\nprint_config()\n</pre> import glob import logging import os from pathlib import Path import shutil import sys import tempfile  import nibabel as nib import numpy as np from monai.config import print_config from monai.data import (     ArrayDataset,     create_test_image_3d,     decollate_batch,     DataLoader ) from monai.handlers import (     MeanDice,     StatsHandler,     TensorBoardImageHandler,     TensorBoardStatsHandler, ) from monai.losses import DiceLoss from monai.networks.nets import UNet from monai.transforms import (     Activations,     EnsureChannelFirst,     AsDiscrete,     Compose,     LoadImage,     RandSpatialCrop,     Resize,     ScaleIntensity, ) from monai.utils import first  import wandb from wandb_addons.monai import WandbStatsHandler, WandbModelCheckpointHandler  import ignite import torch from tqdm.auto import tqdm  print_config() In\u00a0[\u00a0]: Copied! <pre>directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\nroot_dir = tempfile.mkdtemp() if directory is None else directory\nprint(root_dir)\n</pre> directory = os.environ.get(\"MONAI_DATA_DIRECTORY\") root_dir = tempfile.mkdtemp() if directory is None else directory print(root_dir) In\u00a0[\u00a0]: Copied! <pre>log_dir = os.path.join(root_dir, \"logs\")\nwandb.tensorboard.patch(log_dir)\nwandb.init(project=\"monai-integration\", save_code=True, sync_tensorboard=True)\n</pre> log_dir = os.path.join(root_dir, \"logs\") wandb.tensorboard.patch(log_dir) wandb.init(project=\"monai-integration\", save_code=True, sync_tensorboard=True) In\u00a0[\u00a0]: Copied! <pre>logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n</pre> logging.basicConfig(stream=sys.stdout, level=logging.INFO) In\u00a0[\u00a0]: Copied! <pre>for i in tqdm(range(40)):\n    im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)\n\n    n = nib.Nifti1Image(im, np.eye(4))\n    nib.save(n, os.path.join(root_dir, f\"im{i}.nii.gz\"))\n\n    n = nib.Nifti1Image(seg, np.eye(4))\n    nib.save(n, os.path.join(root_dir, f\"seg{i}.nii.gz\"))\n\nimages = sorted(glob.glob(os.path.join(root_dir, \"im*.nii.gz\")))\nsegs = sorted(glob.glob(os.path.join(root_dir, \"seg*.nii.gz\")))\n</pre> for i in tqdm(range(40)):     im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1)      n = nib.Nifti1Image(im, np.eye(4))     nib.save(n, os.path.join(root_dir, f\"im{i}.nii.gz\"))      n = nib.Nifti1Image(seg, np.eye(4))     nib.save(n, os.path.join(root_dir, f\"seg{i}.nii.gz\"))  images = sorted(glob.glob(os.path.join(root_dir, \"im*.nii.gz\"))) segs = sorted(glob.glob(os.path.join(root_dir, \"seg*.nii.gz\"))) In\u00a0[\u00a0]: Copied! <pre># Define transforms for image and segmentation\nimtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        ScaleIntensity(),\n        EnsureChannelFirst(),\n        RandSpatialCrop((96, 96, 96), random_size=False),\n    ]\n)\nsegtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        EnsureChannelFirst(),\n        RandSpatialCrop((96, 96, 96), random_size=False),\n    ]\n)\n\n# Define nifti dataset, dataloader\nds = ArrayDataset(images, imtrans, segs, segtrans)\nloader = DataLoader(ds, batch_size=10, num_workers=2, pin_memory=torch.cuda.is_available())\nim, seg = first(loader)\nprint(im.shape, seg.shape)\n</pre> # Define transforms for image and segmentation imtrans = Compose(     [         LoadImage(image_only=True),         ScaleIntensity(),         EnsureChannelFirst(),         RandSpatialCrop((96, 96, 96), random_size=False),     ] ) segtrans = Compose(     [         LoadImage(image_only=True),         EnsureChannelFirst(),         RandSpatialCrop((96, 96, 96), random_size=False),     ] )  # Define nifti dataset, dataloader ds = ArrayDataset(images, imtrans, segs, segtrans) loader = DataLoader(ds, batch_size=10, num_workers=2, pin_memory=torch.cuda.is_available()) im, seg = first(loader) print(im.shape, seg.shape) In\u00a0[\u00a0]: Copied! <pre># Create UNet, DiceLoss and Adam optimizer\ndevice = torch.device(\"cuda:0\")\nnet = UNet(\n    spatial_dims=3,\n    in_channels=1,\n    out_channels=1,\n    channels=(16, 32, 64, 128, 256),\n    strides=(2, 2, 2, 2),\n    num_res_units=2,\n).to(device)\n\nloss = DiceLoss(sigmoid=True)\nlr = 1e-3\nopt = torch.optim.Adam(net.parameters(), lr)\n</pre> # Create UNet, DiceLoss and Adam optimizer device = torch.device(\"cuda:0\") net = UNet(     spatial_dims=3,     in_channels=1,     out_channels=1,     channels=(16, 32, 64, 128, 256),     strides=(2, 2, 2, 2),     num_res_units=2, ).to(device)  loss = DiceLoss(sigmoid=True) lr = 1e-3 opt = torch.optim.Adam(net.parameters(), lr) In\u00a0[\u00a0]: Copied! <pre># Create trainer\ntrainer = ignite.engine.create_supervised_trainer(net, opt, loss, device, False)\n</pre> # Create trainer trainer = ignite.engine.create_supervised_trainer(net, opt, loss, device, False) In\u00a0[\u00a0]: Copied! <pre># optional section for checkpoint and tensorboard logging\n# adding checkpoint handler to save models (network\n# params and optimizer stats) during training\nlog_dir = os.path.join(root_dir, \"logs\")\ncheckpoint_handler = ignite.handlers.ModelCheckpoint(log_dir, \"net\", n_saved=10, require_empty=False)\ntrainer.add_event_handler(\n    event_name=ignite.engine.Events.EPOCH_COMPLETED,\n    handler=checkpoint_handler,\n    to_save={\"net\": net, \"opt\": opt},\n)\n\n# StatsHandler prints loss at every iteration\n# user can also customize print functions and can use output_transform to convert\n# engine.state.output if it's not a loss value\ntrain_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x)\ntrain_stats_handler.attach(trainer)\n\n# TensorBoardStatsHandler plots loss at every iteration\ntrain_tensorboard_stats_handler = TensorBoardStatsHandler(log_dir=log_dir, output_transform=lambda x: x)\ntrain_tensorboard_stats_handler.attach(trainer)\n\n# WandbStatsHandler logs loss at every iteration to Weights &amp; Biases\ntrain_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x)\ntrain_wandb_stats_handler.attach(trainer)\n\n# CheckpointHandler with `WandbModelCheckpointSaver` logs model\n# checkpoints at every iteration\ncheckpoint_handler = Checkpoint(\n    {\"model\": net, \"optimizer\": opt},\n    WandbModelCheckpointSaver(),\n    n_saved=1,\n    filename_prefix=\"best_checkpoint\",\n    score_name=metric_name,\n    global_step_transform=global_step_from_engine(trainer)\n)\nevaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n</pre> # optional section for checkpoint and tensorboard logging # adding checkpoint handler to save models (network # params and optimizer stats) during training log_dir = os.path.join(root_dir, \"logs\") checkpoint_handler = ignite.handlers.ModelCheckpoint(log_dir, \"net\", n_saved=10, require_empty=False) trainer.add_event_handler(     event_name=ignite.engine.Events.EPOCH_COMPLETED,     handler=checkpoint_handler,     to_save={\"net\": net, \"opt\": opt}, )  # StatsHandler prints loss at every iteration # user can also customize print functions and can use output_transform to convert # engine.state.output if it's not a loss value train_stats_handler = StatsHandler(name=\"trainer\", output_transform=lambda x: x) train_stats_handler.attach(trainer)  # TensorBoardStatsHandler plots loss at every iteration train_tensorboard_stats_handler = TensorBoardStatsHandler(log_dir=log_dir, output_transform=lambda x: x) train_tensorboard_stats_handler.attach(trainer)  # WandbStatsHandler logs loss at every iteration to Weights &amp; Biases train_wandb_stats_handler = WandbStatsHandler(output_transform=lambda x: x) train_wandb_stats_handler.attach(trainer)  # CheckpointHandler with `WandbModelCheckpointSaver` logs model # checkpoints at every iteration checkpoint_handler = Checkpoint(     {\"model\": net, \"optimizer\": opt},     WandbModelCheckpointSaver(),     n_saved=1,     filename_prefix=\"best_checkpoint\",     score_name=metric_name,     global_step_transform=global_step_from_engine(trainer) ) evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler) In\u00a0[\u00a0]: Copied! <pre># optional section for model validation during training\nvalidation_every_n_epochs = 1\n# Set parameters for validation\nmetric_name = \"Mean_Dice\"\n# add evaluation metric to the evaluator engine\nval_metrics = {metric_name: MeanDice()}\npost_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\npost_label = Compose([AsDiscrete(threshold=0.5)])\n# Ignite evaluator expects batch=(img, seg) and\n# returns output=(y_pred, y) at every iteration,\n# user can add output_transform to return other values\nevaluator = ignite.engine.create_supervised_evaluator(\n    net,\n    val_metrics,\n    device,\n    True,\n    output_transform=lambda x, y, y_pred: (\n        [post_pred(i) for i in decollate_batch(y_pred)],\n        [post_label(i) for i in decollate_batch(y)],\n    ),\n)\n\n# create a validation data loader\nval_imtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        ScaleIntensity(),\n        EnsureChannelFirst(),\n        Resize((96, 96, 96)),\n    ]\n)\nval_segtrans = Compose(\n    [\n        LoadImage(image_only=True),\n        EnsureChannelFirst(),\n        Resize((96, 96, 96)),\n    ]\n)\nval_ds = ArrayDataset(images[21:], val_imtrans, segs[21:], val_segtrans)\nval_loader = DataLoader(val_ds, batch_size=5, num_workers=8, pin_memory=torch.cuda.is_available())\n\n\n@trainer.on(ignite.engine.Events.EPOCH_COMPLETED(every=validation_every_n_epochs))\ndef run_validation(engine):\n    evaluator.run(val_loader)\n\n\n# Add stats event handler to print validation stats via evaluator\nval_stats_handler = StatsHandler(\n    name=\"evaluator\",\n    # no need to print loss value, so disable per iteration output\n    output_transform=lambda x: None,\n    # fetch global epoch number from trainer\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_stats_handler.attach(evaluator)\n\n# add handler to record metrics to TensorBoard at every validation epoch\nval_tensorboard_stats_handler = TensorBoardStatsHandler(\n    log_dir=log_dir,\n    # no need to plot loss value, so disable per iteration output\n    output_transform=lambda x: None,\n    # fetch global epoch number from trainer\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_tensorboard_stats_handler.attach(evaluator)\n\nval_wandb_stats_handler = WandbStatsHandler(\n    output_transform=lambda x: None,\n    global_epoch_transform=lambda x: trainer.state.epoch,\n)\nval_wandb_stats_handler.attach(evaluator)\n\n# add handler to draw the first image and the corresponding\n# label and model output in the last batch\n# here we draw the 3D output as GIF format along Depth\n# axis, at every validation epoch\nval_tensorboard_image_handler = TensorBoardImageHandler(\n    log_dir=log_dir,\n    batch_transform=lambda batch: (batch[0], batch[1]),\n    output_transform=lambda output: output[0],\n    global_iter_transform=lambda x: trainer.state.epoch,\n)\nevaluator.add_event_handler(\n    event_name=ignite.engine.Events.EPOCH_COMPLETED,\n    handler=val_tensorboard_image_handler,\n)\n\n# The `Checkpoint` handler for PyTorch Ignite along with `WandbModelCheckpointSaver()`\n# logs model checkpoints as WandB Artifacts.\ncheckpoint_handler = Checkpoint(\n    {\"model\": model, \"optimizer\": optimizer},\n    WandbModelCheckpointSaver(),\n    n_saved=1,\n    filename_prefix=\"best_checkpoint\",\n    score_name=metric_name,\n    global_step_transform=global_step_from_engine(trainer)\n)\nevaluator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n</pre> # optional section for model validation during training validation_every_n_epochs = 1 # Set parameters for validation metric_name = \"Mean_Dice\" # add evaluation metric to the evaluator engine val_metrics = {metric_name: MeanDice()} post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)]) post_label = Compose([AsDiscrete(threshold=0.5)]) # Ignite evaluator expects batch=(img, seg) and # returns output=(y_pred, y) at every iteration, # user can add output_transform to return other values evaluator = ignite.engine.create_supervised_evaluator(     net,     val_metrics,     device,     True,     output_transform=lambda x, y, y_pred: (         [post_pred(i) for i in decollate_batch(y_pred)],         [post_label(i) for i in decollate_batch(y)],     ), )  # create a validation data loader val_imtrans = Compose(     [         LoadImage(image_only=True),         ScaleIntensity(),         EnsureChannelFirst(),         Resize((96, 96, 96)),     ] ) val_segtrans = Compose(     [         LoadImage(image_only=True),         EnsureChannelFirst(),         Resize((96, 96, 96)),     ] ) val_ds = ArrayDataset(images[21:], val_imtrans, segs[21:], val_segtrans) val_loader = DataLoader(val_ds, batch_size=5, num_workers=8, pin_memory=torch.cuda.is_available())   @trainer.on(ignite.engine.Events.EPOCH_COMPLETED(every=validation_every_n_epochs)) def run_validation(engine):     evaluator.run(val_loader)   # Add stats event handler to print validation stats via evaluator val_stats_handler = StatsHandler(     name=\"evaluator\",     # no need to print loss value, so disable per iteration output     output_transform=lambda x: None,     # fetch global epoch number from trainer     global_epoch_transform=lambda x: trainer.state.epoch, ) val_stats_handler.attach(evaluator)  # add handler to record metrics to TensorBoard at every validation epoch val_tensorboard_stats_handler = TensorBoardStatsHandler(     log_dir=log_dir,     # no need to plot loss value, so disable per iteration output     output_transform=lambda x: None,     # fetch global epoch number from trainer     global_epoch_transform=lambda x: trainer.state.epoch, ) val_tensorboard_stats_handler.attach(evaluator)  val_wandb_stats_handler = WandbStatsHandler(     output_transform=lambda x: None,     global_epoch_transform=lambda x: trainer.state.epoch, ) val_wandb_stats_handler.attach(evaluator)  # add handler to draw the first image and the corresponding # label and model output in the last batch # here we draw the 3D output as GIF format along Depth # axis, at every validation epoch val_tensorboard_image_handler = TensorBoardImageHandler(     log_dir=log_dir,     batch_transform=lambda batch: (batch[0], batch[1]),     output_transform=lambda output: output[0],     global_iter_transform=lambda x: trainer.state.epoch, ) evaluator.add_event_handler(     event_name=ignite.engine.Events.EPOCH_COMPLETED,     handler=val_tensorboard_image_handler, )  # The `Checkpoint` handler for PyTorch Ignite along with `WandbModelCheckpointSaver()` # logs model checkpoints as WandB Artifacts. checkpoint_handler = Checkpoint(     {\"model\": model, \"optimizer\": optimizer},     WandbModelCheckpointSaver(),     n_saved=1,     filename_prefix=\"best_checkpoint\",     score_name=metric_name,     global_step_transform=global_step_from_engine(trainer) ) evaluator.add_event_handler(Events.COMPLETED, checkpoint_handler) In\u00a0[\u00a0]: Copied! <pre># create a training data loader\ntrain_ds = ArrayDataset(images[:20], imtrans, segs[:20], segtrans)\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=5,\n    shuffle=True,\n    num_workers=8,\n    pin_memory=torch.cuda.is_available(),\n)\n\nmax_epochs = 10\nstate = trainer.run(train_loader, max_epochs)\n</pre> # create a training data loader train_ds = ArrayDataset(images[:20], imtrans, segs[:20], segtrans) train_loader = DataLoader(     train_ds,     batch_size=5,     shuffle=True,     num_workers=8,     pin_memory=torch.cuda.is_available(), )  max_epochs = 10 state = trainer.run(train_loader, max_epochs) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n\nif directory is None:\n    shutil.rmtree(root_dir)\n</pre> wandb.finish()  if directory is None:     shutil.rmtree(root_dir)"},{"location":"monai/examples/unet_3d_segmentation/#setup-environment","title":"Setup environment\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-weights-biases-run","title":"Setup Weights &amp; Biases run\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-logging","title":"Setup logging\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-demo-data","title":"Setup demo data\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-transforms-dataset","title":"Setup transforms, dataset\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#create-model-loss-optimizer","title":"Create Model, Loss, Optimizer\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#create-supervised_trainer-using-ignite","title":"Create supervised_trainer using ignite\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#setup-event-handlers-for-checkpointing-and-logging","title":"Setup event handlers for checkpointing and logging\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#add-validation-every-n-epochs","title":"Add Validation every N epochs\u00b6","text":""},{"location":"monai/examples/unet_3d_segmentation/#run-training-loop","title":"Run training loop\u00b6","text":""},{"location":"prompts/tracer/","title":"Trace","text":"<p>A high level implementation of <code>Trace</code> for Weight &amp; Biases Prompts.</p>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace","title":"<code>Trace</code>","text":"<p>Manage and log a trace - a collection of spans their metadata and hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>(str) The name of the root span.</p> required <code>kind</code> <code>str</code> <p>(str, optional) The kind of the root span.</p> <code>None</code> <code>status_code</code> <code>str</code> <p>(str, optional) The status of the root span, either \"error\" or \"success\".</p> <code>None</code> <code>status_message</code> <code>str</code> <p>(str, optional) Any status message associated with the root span.</p> <code>None</code> <code>metadata</code> <code>dict</code> <p>(dict, optional) Any additional metadata for the root span.</p> <code>None</code> <code>start_time_ms</code> <code>int</code> <p>(int, optional) The start time of the root span in milliseconds.</p> <code>None</code> <code>end_time_ms</code> <code>int</code> <p>(int, optional) The end time of the root span in milliseconds.</p> <code>None</code> <code>inputs</code> <code>dict</code> <p>(dict, optional) The named inputs of the root span.</p> <code>None</code> <code>outputs</code> <code>dict</code> <p>(dict, optional) The named outputs of the root span.</p> <code>None</code> <code>model_dict</code> <code>dict</code> <p>(dict, optional) A json serializable dictionary containing the model architecture details.</p> <code>None</code> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>class Trace:\n\"\"\"Manage and log a trace - a collection of spans their metadata and hierarchy.\n    Arguments:\n        name: (str) The name of the root span.\n        kind: (str, optional) The kind of the root span.\n        status_code: (str, optional) The status of the root span, either \"error\" or \"success\".\n        status_message: (str, optional) Any status message associated with the root span.\n        metadata: (dict, optional) Any additional metadata for the root span.\n        start_time_ms: (int, optional) The start time of the root span in milliseconds.\n        end_time_ms: (int, optional) The end time of the root span in milliseconds.\n        inputs: (dict, optional) The named inputs of the root span.\n        outputs: (dict, optional) The named outputs of the root span.\n        model_dict: (dict, optional) A json serializable dictionary containing the model architecture details.\n    \"\"\"\ndef __init__(\nself,\nname: str,\nkind: str = None,\nstatus_code: str = None,\nstatus_message: str = None,\nmetadata: dict = None,\nstart_time_ms: int = None,\nend_time_ms: int = None,\ninputs: dict = None,\noutputs: dict = None,\nmodel_dict: dict = None,\n):\nself._span = self._assert_and_create_span(\nname=name,\nkind=kind,\nstatus_code=status_code,\nstatus_message=status_message,\nmetadata=metadata,\nstart_time_ms=start_time_ms,\nend_time_ms=end_time_ms,\ninputs=inputs,\noutputs=outputs,\n)\nif model_dict is not None:\nassert isinstance(model_dict, dict), \"Model dict must be a dictionary\"\nself._model_dict = model_dict\ndef _assert_and_create_span(\nself,\nname: str,\nkind: Optional[str] = None,\nstatus_code: Optional[str] = None,\nstatus_message: Optional[str] = None,\nmetadata: Optional[dict] = None,\nstart_time_ms: Optional[int] = None,\nend_time_ms: Optional[int] = None,\ninputs: Optional[dict] = None,\noutputs: Optional[dict] = None,\n):\nif kind is not None:\nassert (\nkind.upper() in SpanKind.__members__\n), \"Invalid span kind, can be one of 'LLM', 'AGENT', 'CHAIN', 'TOOL'\"\nkind = SpanKind(kind.upper())\nif status_code is not None:\nassert (\nstatus_code.upper() in StatusCode.__members__\n), \"Invalid status code, can be one of 'SUCCESS' or 'ERROR'\"\nstatus_code = StatusCode(status_code.upper())\nif inputs is not None and outputs is not None:\nassert isinstance(inputs, dict), \"Inputs must be a dictionary\"\nassert isinstance(outputs, dict), \"Outputs must be a dictionary\"\nresult = Result(inputs=inputs, outputs=outputs)\nelse:\nresult = None\nreturn Span(\nname=name,\nspan_kind=kind,\nstatus_code=status_code,\nstatus_message=status_message,\nattributes=metadata,\nstart_time_ms=start_time_ms,\nend_time_ms=end_time_ms,\nresults=[result],\n)\ndef add_child(\nself,\nchild: \"Trace\",\n) -&gt; \"Trace\":\n\"\"\"Add a child span to the current span of the trace.\"\"\"\nself._span.add_child_span(child._span)\nif self._model_dict is not None and child._model_dict is not None:\nself._model_dict.update({child._span.name: child._model_dict})\nreturn self\ndef add_metadata(self, metadata: dict) -&gt; \"Trace\":\n\"\"\"Add metadata to the span of the current trace.\"\"\"\nif self._span.attributes is None:\nself._span.attributes = metadata\nelse:\nself._span.attributes.update(metadata)\nreturn self\ndef add_inputs_and_outputs(self, inputs: dict, outputs: dict) -&gt; \"Trace\":\n\"\"\"Add a result to the span of the current trace.\"\"\"\nif self._span.results == [None]:\nresult = Result(inputs=inputs, outputs=outputs)\nself._span.results = [result]\nelse:\nresult = Result(inputs=inputs, outputs=outputs)\nself._span.results.append(result)\nreturn self\ndef log(self, name: str) -&gt; None:\n\"\"\"Log the trace to a wandb run\"\"\"\ntrace_tree = WBTraceTree(self._span, self._model_dict)\nassert (\nwandb.run is not None\n), \"You must call wandb.init() before logging a trace\"\nwandb.run.log({name: trace_tree})\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.add_child","title":"<code>add_child(child)</code>","text":"<p>Add a child span to the current span of the trace.</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def add_child(\nself,\nchild: \"Trace\",\n) -&gt; \"Trace\":\n\"\"\"Add a child span to the current span of the trace.\"\"\"\nself._span.add_child_span(child._span)\nif self._model_dict is not None and child._model_dict is not None:\nself._model_dict.update({child._span.name: child._model_dict})\nreturn self\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.add_inputs_and_outputs","title":"<code>add_inputs_and_outputs(inputs, outputs)</code>","text":"<p>Add a result to the span of the current trace.</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def add_inputs_and_outputs(self, inputs: dict, outputs: dict) -&gt; \"Trace\":\n\"\"\"Add a result to the span of the current trace.\"\"\"\nif self._span.results == [None]:\nresult = Result(inputs=inputs, outputs=outputs)\nself._span.results = [result]\nelse:\nresult = Result(inputs=inputs, outputs=outputs)\nself._span.results.append(result)\nreturn self\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.add_metadata","title":"<code>add_metadata(metadata)</code>","text":"<p>Add metadata to the span of the current trace.</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def add_metadata(self, metadata: dict) -&gt; \"Trace\":\n\"\"\"Add metadata to the span of the current trace.\"\"\"\nif self._span.attributes is None:\nself._span.attributes = metadata\nelse:\nself._span.attributes.update(metadata)\nreturn self\n</code></pre>"},{"location":"prompts/tracer/#wandb_addons.prompts.tracer.Trace.log","title":"<code>log(name)</code>","text":"<p>Log the trace to a wandb run</p> Source code in <code>wandb_addons/prompts/tracer.py</code> <pre><code>def log(self, name: str) -&gt; None:\n\"\"\"Log the trace to a wandb run\"\"\"\ntrace_tree = WBTraceTree(self._span, self._model_dict)\nassert (\nwandb.run is not None\n), \"You must call wandb.init() before logging a trace\"\nwandb.run.log({name: trace_tree})\n</code></pre>"},{"location":"prompts/examples/Trace_QuickStart/","title":"Trace QuickStart","text":"<p>A quick start example that demonstrates how to use the <code>Trace</code> class, a high-level API to log LLM calls with the wandb prompts feature.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -qqq -U openai langchain wandb\n</pre> !pip install -qqq -U openai langchain wandb In\u00a0[\u00a0]: Copied! <pre>import datetime\n\nimport wandb\nfrom langchain.chains import LLMChain\nfrom langchain.llms.fake import FakeListLLM\nfrom langchain.prompts import PromptTemplate\n\nfrom wandb_addons.prompts import Trace\n</pre> import datetime  import wandb from langchain.chains import LLMChain from langchain.llms.fake import FakeListLLM from langchain.prompts import PromptTemplate  from wandb_addons.prompts import Trace In\u00a0[\u00a0]: Copied! <pre>PROJECT=\"high_level_trace\"\n</pre> PROJECT=\"high_level_trace\" In\u00a0[\u00a0]: Copied! <pre>#trace langchain chains\nrun = wandb.init(project=PROJECT)\nllm = FakeListLLM(responses=[f\"Fake response: {i}\" for i in range(100)])\nprompt_template = \"What is a good name for a company that makes {product}?\"\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=prompt_template,\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nfor i in range(2):\n    product = f\"q: {i} - {datetime.datetime.now().timestamp()}\"\n    start_time_ms = datetime.datetime.now().timestamp() * 1000\n    response = chain(product)\n    end_time_ms = datetime.datetime.now().timestamp() * 1000\n    trace = Trace(name=f\"fake_chain_{i}\",\n          kind=\"chain\",\n          status_code=\"success\",\n          metadata=None,\n          start_time_ms=start_time_ms,\n          end_time_ms=end_time_ms,\n          inputs={\"prompt\":prompt_template.format(product=product)},\n          outputs={\"response\": response[\"text\"]},\n          )\n    trace.log(name=f\"trace_{i}\")\nrun.finish()\n</pre> #trace langchain chains run = wandb.init(project=PROJECT) llm = FakeListLLM(responses=[f\"Fake response: {i}\" for i in range(100)]) prompt_template = \"What is a good name for a company that makes {product}?\" prompt = PromptTemplate(     input_variables=[\"product\"],     template=prompt_template, )  chain = LLMChain(llm=llm, prompt=prompt)  for i in range(2):     product = f\"q: {i} - {datetime.datetime.now().timestamp()}\"     start_time_ms = datetime.datetime.now().timestamp() * 1000     response = chain(product)     end_time_ms = datetime.datetime.now().timestamp() * 1000     trace = Trace(name=f\"fake_chain_{i}\",           kind=\"chain\",           status_code=\"success\",           metadata=None,           start_time_ms=start_time_ms,           end_time_ms=end_time_ms,           inputs={\"prompt\":prompt_template.format(product=product)},           outputs={\"response\": response[\"text\"]},           )     trace.log(name=f\"trace_{i}\") run.finish() In\u00a0[\u00a0]: Copied! <pre># trace openai api calls\nfrom getpass import getpass\nimport openai\n\nopenai.api_key = getpass(\"Please enter your openai api key\")\n</pre> # trace openai api calls from getpass import getpass import openai  openai.api_key = getpass(\"Please enter your openai api key\") In\u00a0[\u00a0]: Copied! <pre>run = wandb.init(project=PROJECT)\nrequest_kwargs = dict(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\n            \"role\": \"assistant\",\n            \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",\n        },\n        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n    ],\n)\n\nstart_time_ms = datetime.datetime.now().timestamp() * 1000\nresponse = openai.ChatCompletion.create(**request_kwargs)\nend_time_ms = datetime.datetime.now().timestamp() * 1000\n\ntrace = Trace(\n    name=f\"openai_chat_completion\",\n    kind=\"llm\",\n    status_code=\"success\",\n    metadata={\"model\": \"gpt-3.5-turbo\"},\n    start_time_ms=start_time_ms,\n    end_time_ms=end_time_ms,\n    inputs={\"messages\":request_kwargs[\"messages\"]},\n    outputs={\"response\": response.choices[0][\"message\"][\"content\"]},\n)\n\ntrace.log(name=f\"openai_trace\")\nrun.finish()\ndisplay(run)\n</pre> run = wandb.init(project=PROJECT) request_kwargs = dict(     model=\"gpt-3.5-turbo\",     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},         {             \"role\": \"assistant\",             \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\",         },         {\"role\": \"user\", \"content\": \"Where was it played?\"},     ], )  start_time_ms = datetime.datetime.now().timestamp() * 1000 response = openai.ChatCompletion.create(**request_kwargs) end_time_ms = datetime.datetime.now().timestamp() * 1000  trace = Trace(     name=f\"openai_chat_completion\",     kind=\"llm\",     status_code=\"success\",     metadata={\"model\": \"gpt-3.5-turbo\"},     start_time_ms=start_time_ms,     end_time_ms=end_time_ms,     inputs={\"messages\":request_kwargs[\"messages\"]},     outputs={\"response\": response.choices[0][\"message\"][\"content\"]}, )  trace.log(name=f\"openai_trace\") run.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># use with promprtify\n!pip install -qqq -U promptify\n</pre> # use with promprtify !pip install -qqq -U promptify In\u00a0[\u00a0]: Copied! <pre>from promptify import OpenAI\nfrom promptify import Prompter\n\nrun = wandb.init(project=PROJECT)\n\n# NER example\nsentence = \"The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection\"\n\nmodel = OpenAI(openai.api_key) # or `HubModel()` for Huggingface-based inference\nnlp_prompter = Prompter(model)\n\nstart_time_ms = datetime.datetime.now().timestamp() * 1000\nresult = nlp_prompter.fit(\n    'ner.jinja',\n    domain='medical',\n    text_input=sentence,\n    labels=None)\nend_time_ms = datetime.datetime.now().timestamp() * 1000\n\n\ntrace = Trace(\n    name=f\"openai_chat_completion\",\n    kind=\"llm\",\n    status_code=\"success\",\n    metadata={k:v for k,v in result.items() if k != \"text\"},\n    start_time_ms=start_time_ms,\n    end_time_ms=end_time_ms,\n    inputs={\"sentence\":sentence},\n    outputs={\"entities\": result[\"text\"]},\n)\ntrace.log(name=\"promptify_ner\")\nrun.finish()\ndisplay(run)\n</pre> from promptify import OpenAI from promptify import Prompter  run = wandb.init(project=PROJECT)  # NER example sentence = \"The patient is a 93-year-old female with a medical history of chronic right hip pain, osteoporosis, hypertension, depression, and chronic atrial fibrillation admitted for evaluation and management of severe nausea and vomiting and urinary tract infection\"  model = OpenAI(openai.api_key) # or `HubModel()` for Huggingface-based inference nlp_prompter = Prompter(model)  start_time_ms = datetime.datetime.now().timestamp() * 1000 result = nlp_prompter.fit(     'ner.jinja',     domain='medical',     text_input=sentence,     labels=None) end_time_ms = datetime.datetime.now().timestamp() * 1000   trace = Trace(     name=f\"openai_chat_completion\",     kind=\"llm\",     status_code=\"success\",     metadata={k:v for k,v in result.items() if k != \"text\"},     start_time_ms=start_time_ms,     end_time_ms=end_time_ms,     inputs={\"sentence\":sentence},     outputs={\"entities\": result[\"text\"]}, ) trace.log(name=\"promptify_ner\") run.finish() display(run) In\u00a0[\u00a0]: Copied! <pre>!pip install -qqq -U guidance\n</pre> !pip install -qqq -U guidance In\u00a0[\u00a0]: Copied! <pre>import guidance\n\nrun = wandb.init(project=PROJECT)\n# define the model we will use\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\", api_key=openai.api_key)\n\n# define the few shot examples\nexamples = [\n    {'input': 'I wrote about shakespeare',\n    'entities': [{'entity': 'I', 'time': 'present'}, {'entity': 'Shakespeare', 'time': '16th century'}],\n    'reasoning': 'I can write about Shakespeare because he lived in the past with respect to me.',\n    'answer': 'No'},\n    {'input': 'Shakespeare wrote about me',\n    'entities': [{'entity': 'Shakespeare', 'time': '16th century'}, {'entity': 'I', 'time': 'present'}],\n    'reasoning': 'Shakespeare cannot have written about me, because he died before I was born',\n    'answer': 'Yes'}\n]\n\n# define the guidance program\nstructure_prompt = guidance(\n'''Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities).\n----\n\n{{~! display the few-shot examples ~}}\n{{~#each examples}}\nSentence: {{this.input}}\nEntities and dates:{{#each this.entities}}\n{{this.entity}}: {{this.time}}{{/each}}\nReasoning: {{this.reasoning}}\nAnachronism: {{this.answer}}\n---\n{{~/each}}\n\n{{~! place the real question at the end }}\nSentence: {{input}}\nEntities and dates:\n{{gen \"entities\"}}\nReasoning:{{gen \"Reasoning\"}}\nAnachronism:{{#select \"answer\"}} Yes{{or}} No{{/select}}''')\n\nstart_time_ms = datetime.datetime.now().timestamp() * 1000\n# execute the program\nresult = structure_prompt(\n    examples=examples,\n    input='The T-rex bit my dog'\n)\nend_time_ms = datetime.datetime.now().timestamp() * 1000\n# trace guidance\n\ntrace = Trace(\n    name=f\"guidance_anachronism\",\n    kind=\"llm\",\n    status_code=\"success\",\n    metadata=None,\n    start_time_ms=start_time_ms,\n    end_time_ms=end_time_ms,\n    inputs={\"sentence\":result.variables()[\"input\"]},\n    outputs={\"entities\": result.variables()[\"entities\"], \"answer\": result.variables()[\"answer\"]},\n)\ntrace.log(name=\"guidance_anachronism\")\nrun.finish()\ndisplay(run)\n</pre> import guidance  run = wandb.init(project=PROJECT) # define the model we will use guidance.llm = guidance.llms.OpenAI(\"text-davinci-003\", api_key=openai.api_key)  # define the few shot examples examples = [     {'input': 'I wrote about shakespeare',     'entities': [{'entity': 'I', 'time': 'present'}, {'entity': 'Shakespeare', 'time': '16th century'}],     'reasoning': 'I can write about Shakespeare because he lived in the past with respect to me.',     'answer': 'No'},     {'input': 'Shakespeare wrote about me',     'entities': [{'entity': 'Shakespeare', 'time': '16th century'}, {'entity': 'I', 'time': 'present'}],     'reasoning': 'Shakespeare cannot have written about me, because he died before I was born',     'answer': 'Yes'} ]  # define the guidance program structure_prompt = guidance( '''Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities). ----  {{~! display the few-shot examples ~}} {{~#each examples}} Sentence: {{this.input}} Entities and dates:{{#each this.entities}} {{this.entity}}: {{this.time}}{{/each}} Reasoning: {{this.reasoning}} Anachronism: {{this.answer}} --- {{~/each}}  {{~! place the real question at the end }} Sentence: {{input}} Entities and dates: {{gen \"entities\"}} Reasoning:{{gen \"Reasoning\"}} Anachronism:{{#select \"answer\"}} Yes{{or}} No{{/select}}''')  start_time_ms = datetime.datetime.now().timestamp() * 1000 # execute the program result = structure_prompt(     examples=examples,     input='The T-rex bit my dog' ) end_time_ms = datetime.datetime.now().timestamp() * 1000 # trace guidance  trace = Trace(     name=f\"guidance_anachronism\",     kind=\"llm\",     status_code=\"success\",     metadata=None,     start_time_ms=start_time_ms,     end_time_ms=end_time_ms,     inputs={\"sentence\":result.variables()[\"input\"]},     outputs={\"entities\": result.variables()[\"entities\"], \"answer\": result.variables()[\"answer\"]}, ) trace.log(name=\"guidance_anachronism\") run.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># example hierarchies usage in the Trace class\nimport time\n\nroot_trace = Trace(\n    name=\"Parent Model\",\n    kind=\"LLM\",\n    status_code= \"SUCCESS\",\n    metadata={\"attr_1\": 1, \"attr_2\": 2,},\n    start_time_ms=int(round(time.time() * 1000)),\n    end_time_ms=int(round(time.time() * 1000))+1000,\n    inputs={\"user\": \"How old is google?\"},\n    outputs={\"assistant\": \"25 years old\"},\n    model_dict={\"_kind\": \"openai\", \"api_type\": \"azure\"}\n      )\n\nfirst_child = Trace(\n    name=\"Child 1 Model\",\n    kind=\"LLM\",\n    status_code= \"ERROR\",\n    metadata={\"child1_attr_1\": 1, \"child1_attr_2\": 2,},\n    start_time_ms=int(round(time.time() * 1000))+2000,\n    end_time_ms=int(round(time.time() * 1000))+3000,\n    inputs={\"user\": \"How old is google?\"},\n    outputs={\"assistant\": \"25 years old\"},\n    model_dict={\"_kind\": \"openai\", \"api_type\": \"child1_azure\"}\n      )\n\nsecond_child = Trace(\n    name=\"Child 2 Model\",\n    kind=\"LLM\",\n    status_code=\"SUCCESS\",\n    metadata={\"child2_attr_1\": 1, \"child2_attr_2\": 2,},\n    start_time_ms=int(round(time.time() * 1000))+4000,\n    end_time_ms=int(round(time.time() * 1000))+5000,\n    inputs={\"user\": \"How old is google?\"},\n    outputs={\"assistant\": \"25 years old\"},\n    model_dict={\"_kind\": \"openai\", \"api_type\": \"child2_azure\"}\n      )\n</pre> # example hierarchies usage in the Trace class import time  root_trace = Trace(     name=\"Parent Model\",     kind=\"LLM\",     status_code= \"SUCCESS\",     metadata={\"attr_1\": 1, \"attr_2\": 2,},     start_time_ms=int(round(time.time() * 1000)),     end_time_ms=int(round(time.time() * 1000))+1000,     inputs={\"user\": \"How old is google?\"},     outputs={\"assistant\": \"25 years old\"},     model_dict={\"_kind\": \"openai\", \"api_type\": \"azure\"}       )  first_child = Trace(     name=\"Child 1 Model\",     kind=\"LLM\",     status_code= \"ERROR\",     metadata={\"child1_attr_1\": 1, \"child1_attr_2\": 2,},     start_time_ms=int(round(time.time() * 1000))+2000,     end_time_ms=int(round(time.time() * 1000))+3000,     inputs={\"user\": \"How old is google?\"},     outputs={\"assistant\": \"25 years old\"},     model_dict={\"_kind\": \"openai\", \"api_type\": \"child1_azure\"}       )  second_child = Trace(     name=\"Child 2 Model\",     kind=\"LLM\",     status_code=\"SUCCESS\",     metadata={\"child2_attr_1\": 1, \"child2_attr_2\": 2,},     start_time_ms=int(round(time.time() * 1000))+4000,     end_time_ms=int(round(time.time() * 1000))+5000,     inputs={\"user\": \"How old is google?\"},     outputs={\"assistant\": \"25 years old\"},     model_dict={\"_kind\": \"openai\", \"api_type\": \"child2_azure\"}       ) In\u00a0[\u00a0]: Copied! <pre># simple heirarchy\nrun = wandb.init(project=PROJECT, job_type=\"simple_heirarchy\")\n\nroot_trace.add_child(first_child)\nfirst_child.add_child(second_child)\n\nroot_trace.log(\"root_trace\")\n\nwandb.finish()\ndisplay(run)\n</pre> # simple heirarchy run = wandb.init(project=PROJECT, job_type=\"simple_heirarchy\")  root_trace.add_child(first_child) first_child.add_child(second_child)  root_trace.log(\"root_trace\")  wandb.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># nested heirarchy\nrun = wandb.init(project=PROJECT, job_type=\"nested_heirarchy\")\n\nroot_trace.add_child(first_child)\nfirst_child.add_child(second_child)\nroot_trace.add_child(second_child)\n\nroot_trace.log(\"root_trace\")\n\nwandb.finish()\ndisplay(run)\n</pre> # nested heirarchy run = wandb.init(project=PROJECT, job_type=\"nested_heirarchy\")  root_trace.add_child(first_child) first_child.add_child(second_child) root_trace.add_child(second_child)  root_trace.log(\"root_trace\")  wandb.finish() display(run) In\u00a0[\u00a0]: Copied! <pre># all traces\nrun = wandb.init(project=PROJECT, job_type=\"all_traces\")\n\nroot_trace.add_child(first_child)\nfirst_child.add_child(second_child)\n\nsecond_child.log(\"second_child\")\nfirst_child.log(\"first_child\")\nroot_trace.log(\"root_trace\")\n\nwandb.finish()\ndisplay(run)\n</pre> # all traces run = wandb.init(project=PROJECT, job_type=\"all_traces\")  root_trace.add_child(first_child) first_child.add_child(second_child)  second_child.log(\"second_child\") first_child.log(\"first_child\") root_trace.log(\"root_trace\")  wandb.finish() display(run)"}]}